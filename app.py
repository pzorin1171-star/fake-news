from flask import Flask, render_template, request, jsonify
import re
import os
from textblob import TextBlob

app = Flask(__name__)

class FakeNewsDetector:
    def __init__(self):
        self.clickbait_words = ['—à–æ–∫', '—Å–µ–Ω—Å–∞—Ü–∏—è', '—Ç–∞–π–Ω–∞', '—Å–∫–∞–Ω–¥–∞–ª', '—Ä–∞–∑–æ–±–ª–∞—á–µ–Ω–∏–µ',
                               '—É–∂–∞—Å', '—á—É–¥–æ', '–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ', '–ø–æ—Ç—Ä—è—Å–∞—é—â–µ', '—Å—Ä–æ—á–Ω–æ',
                               '—ç–∫—Å–∫–ª—é–∑–∏–≤', '—Å–µ–∫—Ä–µ—Ç', '–ø—Ä–∞–≤–¥–∞', '–ª–æ–∂—å', '–æ–±–º–∞–Ω']
        
        self.certainty_words = ['—Ç–æ—á–Ω–æ', '–∞–±—Å–æ–ª—é—Ç–Ω–æ', '–Ω–µ—Å–æ–º–Ω–µ–Ω–Ω–æ', '–±–µ–∑—É—Å–ª–æ–≤–Ω–æ',
                               '–∫–æ–Ω–µ—á–Ω–æ', '—è–≤–Ω–æ', '–æ—á–µ–≤–∏–¥–Ω–æ', '–Ω–∞–≤–µ—Ä–Ω—è–∫–∞',
                               '–≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ', '—Å—Ç–æ–ø—Ä–æ—Ü–µ–Ω—Ç–Ω–æ', '–¥–æ–∫–∞–∑–∞–Ω–æ']
        
        self.formal_words = ['—Å–æ–æ–±—â–∏–ª', '–∑–∞—è–≤–∏–ª', '–æ—Ç–º–µ—Ç–∏–ª', '–ø–æ–¥—á–µ—Ä–∫–Ω—É–ª',
                            '—É–∫–∞–∑–∞–ª', '–¥–æ–±–∞–≤–∏–ª', '–ø–æ –¥–∞–Ω–Ω—ã–º', '—Å–æ–≥–ª–∞—Å–Ω–æ']
        
        self.source_indicators = ['–ø–æ –¥–∞–Ω–Ω—ã–º', '—Å–æ–≥–ª–∞—Å–Ω–æ', '–∫–∞–∫ —Å–æ–æ–±—â–∞–µ—Ç',
                                 '–ø–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏', '–ø–æ —Å–ª–æ–≤–∞–º', '–ø–æ —Å–≤–µ–¥–µ–Ω–∏—è–º']
        
        self.news_sources = ['—Ü–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '–º–∏–Ω–∑–¥—Ä–∞–≤', '—Ä–æ—Å–ø–æ—Ç—Ä–µ–±–Ω–∞–¥–∑–æ—Ä',
                            '—Ä–æ—Å—Å—Ç–∞—Ç', '–æ–æ–Ω', '–≤—Å–µ–º–∏—Ä–Ω—ã–π –±–∞–Ω–∫', '–º–≤—Ñ', '—ç–∫—Å–ø–µ—Ä—Ç—ã', '–∞–Ω–∞–ª–∏—Ç–∏–∫–∏']
    
    def analyze_text(self, text):
        text_lower = text.lower()
        words = re.findall(r'\b\w+\b', text_lower)
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        # 1. –ö–ª–∏–∫–±–µ–π—Ç
        clickbait_count = sum(1 for word in self.clickbait_words if word in text_lower)
        clickbait_score = min(clickbait_count / 3, 1.0)
        
        # 2. –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
        try:
            blob = TextBlob(text)
            emotional_score = abs(blob.sentiment.polarity)
        except:
            emotional_score = 0
        
        # 3. –ö–∞—Ç–µ–≥–æ—Ä–∏—á–Ω–æ—Å—Ç—å
        certainty_count = sum(1 for word in self.certainty_words if word in text_lower)
        certainty_score = min(certainty_count / 3, 1.0)
        
        # 4. –§–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—å
        formal_count = sum(1 for word in self.formal_words if word in text_lower)
        formality_score = min(formal_count / 3, 1.0)
        
        # 5. –ò—Å—Ç–æ—á–Ω–∏–∫–∏
        source_count = sum(1 for word in self.source_indicators if word in text_lower)
        source_score = min(source_count / 2, 1.0)
        
        # 6. –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        news_source_count = sum(1 for source in self.news_sources if source in text_lower)
        news_source_score = min(news_source_count / 2, 1.0)
        
        # 7. –ü—É–Ω–∫—Ç—É–∞—Ü–∏—è
        exclamation_count = text.count('!')
        exclamation_density = exclamation_count / max(len(sentences), 1)
        
        # 8. –†–µ–≥–∏—Å—Ç—Ä
        caps_count = sum(1 for c in text if c.isupper())
        caps_ratio = caps_count / len(text) if len(text) > 0 else 0
        
        # 9. –ß–∏—Å–ª–∞
        has_percentages = 1 if ('%' in text or '–ø—Ä–æ—Ü–µ–Ω—Ç' in text_lower) else 0
        has_numbers = 1 if bool(re.search(r'\d+', text)) else 0
        
        # –†–∞—Å—á–µ—Ç –±–∞–ª–ª–∞ —Ñ–µ–π–∫–æ–≤–æ—Å—Ç–∏
        fake_score = (
            clickbait_score * 0.25 +
            emotional_score * 0.20 +
            certainty_score * 0.15 +
            exclamation_density * 0.15 +
            caps_ratio * 0.10 +
            (1 - formality_score) * 0.10 +
            (1 - source_score) * 0.05
        )
        
        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏
        if has_percentages:
            fake_score *= 0.8
        if news_source_score > 0.3:
            fake_score *= 0.7
        
        # –î–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å (0-100%)
        reliability_score = max(0, min(100, round(100 - (fake_score * 100))))
        
        # –í–µ—Ä–¥–∏–∫—Ç
        if reliability_score >= 75:
            verdict = "–í–´–°–û–ö–ê–Ø –î–û–°–¢–û–í–ï–†–ù–û–°–¢–¨"
            is_fake = False
        elif reliability_score >= 50:
            verdict = "–°–†–ï–î–ù–Ø–Ø –î–û–°–¢–û–í–ï–†–ù–û–°–¢–¨"
            is_fake = fake_score > 0.5
        else:
            verdict = "–ù–ò–ó–ö–ê–Ø –î–û–°–¢–û–í–ï–†–ù–û–°–¢–¨"
            is_fake = True
        
        return {
            'reliability_score': reliability_score,
            'fake_score': round(fake_score * 100, 1),
            'is_fake': is_fake,
            'verdict': verdict,
            'metrics': {
                'clickbait_score': round(clickbait_score * 100),
                'emotional_score': round(emotional_score * 100),
                'certainty_score': round(certainty_score * 100),
                'formality_score': round(formality_score * 100),
                'source_score': round(source_score * 100),
                'news_source_score': round(news_source_score * 100),
                'exclamation_density': round(exclamation_density * 100),
                'caps_ratio': round(caps_ratio * 100)
            },
            'details': {
                'clickbait_words': [w for w in self.clickbait_words if w in text_lower],
                'certainty_words': [w for w in self.certainty_words if w in text_lower],
                'exclamation_count': exclamation_count,
                'has_percentages': bool(has_percentages),
                'has_numbers': bool(has_numbers),
                'word_count': len(words),
                'sentence_count': len(sentences)
            }
        }
    
    def highlight_text(self, text):
        highlighted = text
        
        # –ö–ª–∏–∫–±–µ–π—Ç
        for word in self.clickbait_words:
            pattern = re.compile(f'\\b{word}\\b', re.IGNORECASE)
            highlighted = pattern.sub(
                f'<span class="highlight clickbait">{word.upper()}</span>',
                highlighted
            )
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏—á–Ω–æ—Å—Ç—å
        for word in self.certainty_words:
            pattern = re.compile(f'\\b{word}\\b', re.IGNORECASE)
            highlighted = pattern.sub(
                f'<span class="highlight certainty">{word.upper()}</span>',
                highlighted
            )
        
        # –ò—Å—Ç–æ—á–Ω–∏–∫–∏
        for word in self.source_indicators + self.news_sources:
            pattern = re.compile(f'\\b{word}\\b', re.IGNORECASE)
            highlighted = pattern.sub(
                f'<span class="highlight source">{word.upper()}</span>',
                highlighted
            )
        
        # –ß–∏—Å–ª–∞
        highlighted = re.sub(r'(\d+%?)', r'<span class="highlight number">\1</span>', highlighted)
        
        # –í–æ—Å–∫–ª–∏—Ü–∞–Ω–∏—è
        if '!' in highlighted:
            parts = highlighted.split('!')
            highlighted = ''
            for i, part in enumerate(parts):
                highlighted += part
                if i < len(parts) - 1:
                    highlighted += '<span class="highlight exclamation">!</span>'
        
        return highlighted
    
    def generate_explanations(self, analysis):
        explanations = []
        metrics = analysis['metrics']
        details = analysis['details']
        
        if metrics['clickbait_score'] > 30:
            explanations.append(f"‚ö†Ô∏è –í—ã—Å–æ–∫–∏–π –∫–ª–∏–∫–±–µ–π—Ç-–∏–Ω–¥–µ–∫—Å ({metrics['clickbait_score']}%)")
            if details['clickbait_words']:
                explanations.append(f"   –ù–∞–π–¥–µ–Ω—ã —Å–ª–æ–≤–∞: {', '.join(details['clickbait_words'])}")
        
        if metrics['emotional_score'] > 40:
            explanations.append(f"üò† –í—ã—Å–æ–∫–∞—è —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å ({metrics['emotional_score']}%)")
        
        if metrics['certainty_score'] > 30:
            explanations.append(f"üéØ –ò–∑–±—ã—Ç–æ—á–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—á–Ω–æ—Å—Ç—å ({metrics['certainty_score']}%)")
            if details['certainty_words']:
                explanations.append(f"   –°–ª–æ–≤–∞: {', '.join(details['certainty_words'])}")
        
        if metrics['exclamation_density'] > 30:
            explanations.append(f"‚ùó –ú–Ω–æ–≥–æ –≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏–π ({details['exclamation_count']} —à—Ç.)")
        
        if metrics['caps_ratio'] > 20:
            explanations.append(f"üî† –ú–Ω–æ–≥–æ –∑–∞–≥–ª–∞–≤–Ω—ã—Ö –±—É–∫–≤ ({metrics['caps_ratio']}%)")
        
        if metrics['source_score'] > 30:
            explanations.append(f"‚úÖ –£–ø–æ–º–∏–Ω–∞—é—Ç—Å—è –∏—Å—Ç–æ—á–Ω–∏–∫–∏ ({metrics['source_score']}%)")
        
        if metrics['formality_score'] > 40:
            explanations.append(f"üìù –§–æ—Ä–º–∞–ª—å–Ω—ã–π —Å—Ç–∏–ª—å ({metrics['formality_score']}%)")
        
        if details['has_percentages']:
            explanations.append("üìä –ï—Å—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ")
        
        if metrics['news_source_score'] > 30:
            explanations.append("üèõÔ∏è –£–ø–æ–º–∏–Ω–∞—é—Ç—Å—è –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏")
        
        if not explanations:
            explanations.append("‚úÖ –¢–µ–∫—Å—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —è–≤–Ω—ã—Ö —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –º–∞—Ä–∫–µ—Ä–æ–≤ —Ñ–µ–π–∫–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        
        return explanations

detector = FakeNewsDetector()

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/analyze', methods=['POST'])
def analyze_text():
    try:
        data = request.get_json()
        text = data.get('text', '').strip()
        
        if not text:
            return jsonify({'error': '–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞'})
        
        if len(text) < 20:
            return jsonify({'error': '–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π (–º–∏–Ω–∏–º—É–º 20 —Å–∏–º–≤–æ–ª–æ–≤)'})
        
        if len(text) > 5000:
            return jsonify({'error': '–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π (–º–∞–∫—Å–∏–º—É–º 5000 —Å–∏–º–≤–æ–ª–æ–≤)'})
        
        analysis = detector.analyze_text(text)
        highlighted_text = detector.highlight_text(text)
        explanations = detector.generate_explanations(analysis)
        
        result = {
            'success': True,
            'reliability_score': analysis['reliability_score'],
            'fake_score': analysis['fake_score'],
            'is_fake': analysis['is_fake'],
            'verdict': analysis['verdict'],
            'highlighted_text': highlighted_text,
            'explanations': explanations,
            'metrics': analysis['metrics'],
            'details': analysis['details']
        }
        
        return jsonify(result)
        
    except Exception as e:
        return jsonify({'error': f'–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞: {str(e)}'})

@app.route('/health')
def health_check():
    return jsonify({
        'status': 'ok',
        'version': '1.0',
        'algorithm': 'Rule-based —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑'
    })

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port)
