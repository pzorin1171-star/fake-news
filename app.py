from flask import Flask, render_template, request, jsonify
import re
import os
from textblob import TextBlob
import nltk

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è nltk (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å Python 3.9.0)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)

app = Flask(__name__)

class FakeNewsDetector:
    def __init__(self):
        # –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å–ª–æ–≤–∞—Ä–∏
        self.clickbait_words = ['—à–æ–∫', '—Å–µ–Ω—Å–∞—Ü–∏—è', '—Ç–∞–π–Ω–∞', '—Å–∫–∞–Ω–¥–∞–ª', '—Ä–∞–∑–æ–±–ª–∞—á–µ–Ω–∏–µ',
                               '—É–∂–∞—Å', '—á—É–¥–æ', '–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ', '–ø–æ—Ç—Ä—è—Å–∞—é—â–µ', '—Å—Ä–æ—á–Ω–æ',
                               '—ç–∫—Å–∫–ª—é–∑–∏–≤', '—Å–µ–∫—Ä–µ—Ç', '–ø—Ä–∞–≤–¥–∞', '–ª–æ–∂—å', '–æ–±–º–∞–Ω',
                               '—à–æ–∫–∏—Ä—É—é—â–µ–µ', '–∂—É—Ç–∫–∏–π', '–∞–¥—Å–∫–∏–π', '—á—É–¥–æ–≤–∏—â–Ω—ã–π', '–≥–Ω—É—Å–Ω—ã–π']
        
        self.certainty_words = ['—Ç–æ—á–Ω–æ', '–∞–±—Å–æ–ª—é—Ç–Ω–æ', '–Ω–µ—Å–æ–º–Ω–µ–Ω–Ω–æ', '–±–µ–∑—É—Å–ª–æ–≤–Ω–æ',
                               '–∫–æ–Ω–µ—á–Ω–æ', '—è–≤–Ω–æ', '–æ—á–µ–≤–∏–¥–Ω–æ', '–Ω–∞–≤–µ—Ä–Ω—è–∫–∞',
                               '–≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ', '—Å—Ç–æ–ø—Ä–æ—Ü–µ–Ω—Ç–Ω–æ', '–¥–æ–∫–∞–∑–∞–Ω–æ', '–ø–æ–ª–Ω–æ—Å—Ç—å—é']
        
        self.formal_words = ['—Å–æ–æ–±—â–∏–ª', '–∑–∞—è–≤–∏–ª', '–æ—Ç–º–µ—Ç–∏–ª', '–ø–æ–¥—á–µ—Ä–∫–Ω—É–ª',
                            '—É–∫–∞–∑–∞–ª', '–¥–æ–±–∞–≤–∏–ª', '–ø–æ –¥–∞–Ω–Ω—ã–º', '—Å–æ–≥–ª–∞—Å–Ω–æ', '–∑–∞–º–µ—Ç–∏–ª']
        
        self.source_indicators = ['–ø–æ –¥–∞–Ω–Ω—ã–º', '—Å–æ–≥–ª–∞—Å–Ω–æ', '–∫–∞–∫ —Å–æ–æ–±—â–∞–µ—Ç',
                                 '–ø–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏', '–ø–æ —Å–ª–æ–≤–∞–º', '–ø–æ —Å–≤–µ–¥–µ–Ω–∏—è–º',
                                 '–∏—Å—Ç–æ—á–Ω–∏–∫', '—ç–∫—Å–ø–µ—Ä—Ç', '—É—á–µ–Ω—ã–π', '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ']
        
        self.news_sources = ['—Ü–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '–º–∏–Ω–∑–¥—Ä–∞–≤', '—Ä–æ—Å–ø–æ—Ç—Ä–µ–±–Ω–∞–¥–∑–æ—Ä',
                            '—Ä–æ—Å—Å—Ç–∞—Ç', '–æ–æ–Ω', '–≤—Å–µ–º–∏—Ä–Ω—ã–π –±–∞–Ω–∫', '–º–≤—Ñ', '—ç–∫—Å–ø–µ—Ä—Ç—ã', '–∞–Ω–∞–ª–∏—Ç–∏–∫–∏',
                            '—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç', '–∏–Ω—Å—Ç–∏—Ç—É—Ç', '–ª–∞–±–æ—Ä–∞—Ç–æ—Ä–∏—è', '–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏']
        
        # –ù–æ–≤—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        self.conspiracy_words = ['–∑–∞–≥–æ–≤–æ—Ä', '–≥–ª–æ–±–∞–ª–∏—Å—Ç—ã', '–º–∏—Ä–æ–≤–æ–µ –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ',
                                '–º–∞—Ñ–∏—è', '—Ç–∞–π–Ω—ã–π –∞–ª—å—è–Ω—Å', '—Å–∏–ª—å–Ω—ã–µ –º–∏—Ä–∞ —Å–µ–≥–æ',
                                '—Å–∏—Å—Ç–µ–º–∞', '–∞–≥–µ–Ω—Ç—ã', '–∫—É–ø–ª–µ–Ω—ã', '—Å–∫—Ä—ã–≤–∞—é—Ç',
                                '–∑–∞–º–∞–ª—á–∏–≤–∞—é—Ç', '—Å–æ–∫—Ä—ã—Ç–∏–µ', '–ø—Ä–∞–≤–¥—É —Å–∫—Ä—ã–≤–∞—é—Ç']
        
        self.pseudo_science = ['—Ç–æ–∫—Å–∏–Ω—ã –º—ã—à–ª–µ–Ω–∏—è', '–Ω–µ–π—Ä–æ–Ω—ã –≤—ã–∂–∏–≥–∞–µ—Ç', '–∏–∑–ª—É—á–µ–Ω–∏–µ',
                              '–≤–æ–ª–Ω—ã', '–ø—Ä–æ–≥—Ä–∞–º–º–∞ —É–Ω–∏—á—Ç–æ–∂–µ–Ω–∏—è', '–¥–µ–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π',
                              '–æ—Ä—É–∂–∏–µ –º–∞—Å—Å–æ–≤–æ–≥–æ', '–Ω–µ–π—Ä–æ-—â–∏—Ç', '–±–ª–æ–∫–∏—Ä—É–µ—Ç']
        
        self.fake_stat_words = ['–Ω–∞ 300% –≤—ã—à–µ', '–Ω–∞ 47% —Å–Ω–∏–∂–∞–µ—Ç—Å—è', '99% —Ç–æ–∫—Å–∏–Ω–æ–≤',
                               '–¥–æ–∫–∞–∑–∞–Ω–æ —Ñ–∞–∫—Ç–∞–º–∏', '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ—Ç–æ—Ä—É—é —Å–∫—Ä—ã–≤–∞—é—Ç',
                               '–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω—ã–µ –Ω–æ –¥–æ–∫–∞–∑–∞–Ω–Ω—ã–µ']
        
        self.anonymous_sources = ['–ø—Å–µ–≤–¥–æ–Ω–∏–º', '–∏–º—è –∏–∑–º–µ–Ω–µ–Ω–æ', '–ø–æ–∂–µ–ª–∞–≤—à–∏–π –æ—Å—Ç–∞—Ç—å—Å—è –∞–Ω–æ–Ω–∏–º–Ω—ã–º',
                                 '–Ω–∞—à –∏—Å—Ç–æ—á–Ω–∏–∫', '–Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —ç–∫—Å–ø–µ—Ä—Ç—ã', '—É—á–µ–Ω—ã–π –∫–æ—Ç–æ—Ä—ã–π',
                                 '–¥–æ–∫—Ç–æ—Ä', '—ç–∫—Å–ø–µ—Ä—Ç –ø–æ–¥ –ø—Ä–∏–∫—Ä—ã—Ç–∏–µ–º']
    
    def analyze_text(self, text):
        text_lower = text.lower()
        words = re.findall(r'\b\w+\b', text_lower)
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        # 1. –ö–ª–∏–∫–±–µ–π—Ç
        clickbait_count = sum(1 for word in self.clickbait_words if word in text_lower)
        clickbait_score = min(clickbait_count / 2, 1.0)
        
        # 2. –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
        try:
            blob = TextBlob(text)
            emotional_score = abs(blob.sentiment.polarity)
            if emotional_score > 0.5:
                emotional_score = min(emotional_score * 1.5, 1.0)
        except Exception as e:
            print(f"Sentiment analysis error: {e}")
            emotional_score = 0
        
        # 3. –ö–∞—Ç–µ–≥–æ—Ä–∏—á–Ω–æ—Å—Ç—å
        certainty_count = sum(1 for word in self.certainty_words if word in text_lower)
        certainty_score = min(certainty_count / 2, 1.0)
        
        # 4. –§–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—å
        formal_count = sum(1 for word in self.formal_words if word in text_lower)
        formality_score = min(formal_count / 3, 1.0)
        
        # 5. –ò—Å—Ç–æ—á–Ω–∏–∫–∏
        source_count = sum(1 for word in self.source_indicators if word in text_lower)
        source_score = min(source_count / 2, 1.0)
        
        # 6. –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        news_source_count = sum(1 for source in self.news_sources if source in text_lower)
        news_source_score = min(news_source_count / 2, 1.0)
        
        # 7. –ü—É–Ω–∫—Ç—É–∞—Ü–∏—è
        exclamation_count = text.count('!')
        exclamation_density = exclamation_count / max(len(sentences), 1)
        if exclamation_density > 0.5:
            exclamation_density = 1.0
        
        # 8. –†–µ–≥–∏—Å—Ç—Ä
        caps_count = sum(1 for c in text if c.isupper())
        caps_ratio = caps_count / max(len(text), 1)
        if caps_ratio > 0.1:
            caps_ratio = min(caps_ratio * 2, 1.0)
        
        # 9. –ß–∏—Å–ª–∞ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        has_percentages = 1 if ('%' in text or '–ø—Ä–æ—Ü–µ–Ω—Ç' in text_lower) else 0
        has_numbers = 1 if bool(re.search(r'\d+', text)) else 0
        
        # 10. –ö–æ–Ω—Å–ø–∏—Ä–æ–ª–æ–≥–∏—è
        conspiracy_count = sum(1 for word in self.conspiracy_words if word in text_lower)
        conspiracy_score = min(conspiracy_count / 2, 1.0)
        
        # 11. –ü—Å–µ–≤–¥–æ–Ω–∞—É–∫–∞
        pseudo_science_count = 0
        for phrase in self.pseudo_science:
            if phrase in text_lower:
                pseudo_science_count += 1
        pseudo_science_score = min(pseudo_science_count / 1, 1.0)
        
        # 12. –§–µ–π–∫–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        fake_stat_count = sum(1 for word in self.fake_stat_words if word in text_lower)
        fake_stat_score = min(fake_stat_count / 1, 1.0)
        
        # 13. –ê–Ω–æ–Ω–∏–º–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        anonymous_count = sum(1 for word in self.anonymous_sources if word in text_lower)
        anonymous_score = min(anonymous_count / 1, 1.0)
        
        # –†–∞—Å—á–µ—Ç –±–∞–ª–ª–∞ —Ñ–µ–π–∫–æ–≤–æ—Å—Ç–∏
        fake_score = (
            clickbait_score * 0.20 +
            emotional_score * 0.25 +
            certainty_score * 0.15 +
            exclamation_density * 0.20 +
            caps_ratio * 0.10 +
            (1 - formality_score) * 0.05 +
            conspiracy_score * 0.15 +
            pseudo_science_score * 0.20 +
            fake_stat_score * 0.15 +
            anonymous_score * 0.10
        )
        
        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏
        if has_percentages and fake_stat_score > 0:
            fake_score *= 1.2
        
        if news_source_score > 0.3 and anonymous_score < 0.3:
            fake_score *= 0.6
        elif anonymous_score > 0.3:
            fake_score *= 1.3
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        if clickbait_score > 0.5 and exclamation_density > 0.5:
            fake_score += 0.2
        if conspiracy_score > 0.5 and pseudo_science_score > 0.3:
            fake_score += 0.3
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ 0-1
        fake_score = min(max(fake_score, 0), 1)
        
        # –î–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å (0-100%)
        reliability_score = max(0, min(100, round(100 - (fake_score * 100))))
        
        # –í–µ—Ä–¥–∏–∫—Ç
        if reliability_score >= 80:
            verdict = "–í–´–°–û–ö–ê–Ø –î–û–°–¢–û–í–ï–†–ù–û–°–¢–¨"
            is_fake = False
        elif reliability_score >= 60:
            verdict = "–°–†–ï–î–ù–Ø–Ø –î–û–°–¢–û–í–ï–†–ù–û–°–¢–¨"
            is_fake = fake_score > 0.6
        else:
            verdict = "–ù–ò–ó–ö–ê–Ø –î–û–°–¢–û–í–ï–†–ù–û–°–¢–¨"
            is_fake = True
        
        return {
            'reliability_score': reliability_score,
            'fake_score': round(fake_score * 100, 1),
            'is_fake': is_fake,
            'verdict': verdict,
            'metrics': {
                'clickbait_score': round(clickbait_score * 100),
                'emotional_score': round(emotional_score * 100),
                'certainty_score': round(certainty_score * 100),
                'formality_score': round(formality_score * 100),
                'source_score': round(source_score * 100),
                'news_source_score': round(news_source_score * 100),
                'exclamation_density': round(exclamation_density * 100),
                'caps_ratio': round(caps_ratio * 100),
                'conspiracy_score': round(conspiracy_score * 100),
                'pseudo_science_score': round(pseudo_science_score * 100),
                'fake_stat_score': round(fake_stat_score * 100),
                'anonymous_score': round(anonymous_score * 100)
            },
            'details': {
                'clickbait_words': list(set([w for w in self.clickbait_words if w in text_lower])),
                'certainty_words': list(set([w for w in self.certainty_words if w in text_lower])),
                'conspiracy_words': list(set([w for w in self.conspiracy_words if w in text_lower])),
                'pseudo_science_phrases': list(set([p for p in self.pseudo_science if p in text_lower])),
                'exclamation_count': exclamation_count,
                'has_percentages': bool(has_percentages),
                'has_numbers': bool(has_numbers),
                'word_count': len(words),
                'sentence_count': len(sentences),
                'anonymous_sources_detected': anonymous_count > 0
            }
        }
    
    def highlight_text(self, text):
        highlighted = text
        
        # –ö–ª–∏–∫–±–µ–π—Ç
        for word in self.clickbait_words:
            pattern = re.compile(re.escape(word), re.IGNORECASE)
            highlighted = pattern.sub(
                f'<span class="highlight clickbait">{word.upper()}</span>',
                highlighted
            )
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏—á–Ω–æ—Å—Ç—å
        for word in self.certainty_words:
            pattern = re.compile(re.escape(word), re.IGNORECASE)
            highlighted = pattern.sub(
                f'<span class="highlight certainty">{word.upper()}</span>',
                highlighted
            )
        
        # –ö–æ–Ω—Å–ø–∏—Ä–æ–ª–æ–≥–∏—è
        for word in self.conspiracy_words:
            pattern = re.compile(re.escape(word), re.IGNORECASE)
            highlighted = pattern.sub(
                f'<span class="highlight conspiracy">{word.upper()}</span>',
                highlighted
            )
        
        # –ü—Å–µ–≤–¥–æ–Ω–∞—É—á–Ω—ã–µ —Ñ—Ä–∞–∑—ã
        for phrase in self.pseudo_science:
            pattern = re.compile(re.escape(phrase), re.IGNORECASE)
            highlighted = pattern.sub(
                f'<span class="highlight pseudo">{phrase.upper()}</span>',
                highlighted
            )
        
        # –ò—Å—Ç–æ—á–Ω–∏–∫–∏
        for word in self.source_indicators + self.news_sources + self.anonymous_sources:
            pattern = re.compile(re.escape(word), re.IGNORECASE)
            highlighted = pattern.sub(
                f'<span class="highlight source">{word.upper()}</span>',
                highlighted
            )
        
        # –ß–∏—Å–ª–∞
        highlighted = re.sub(r'(\d+%?)', r'<span class="highlight number">\1</span>', highlighted)
        
        # –í–æ—Å–∫–ª–∏—Ü–∞–Ω–∏—è
        highlighted = re.sub(r'(!+)', r'<span class="highlight exclamation">\1</span>', highlighted)
        
        return highlighted
    
    def generate_explanations(self, analysis):
        explanations = []
        metrics = analysis['metrics']
        details = analysis['details']
        
        if metrics['clickbait_score'] > 20:
            explanations.append(f"‚ö†Ô∏è –í—ã—Å–æ–∫–∏–π –∫–ª–∏–∫–±–µ–π—Ç-–∏–Ω–¥–µ–∫—Å ({metrics['clickbait_score']}%)")
            if details['clickbait_words']:
                words = details['clickbait_words'][:3]
                explanations.append(f"   –ù–∞–π–¥–µ–Ω—ã —Å–ª–æ–≤–∞: {', '.join(words)}")
        
        if metrics['emotional_score'] > 30:
            explanations.append(f"üò† –í—ã—Å–æ–∫–∞—è —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å ({metrics['emotional_score']}%)")
        
        if metrics['certainty_score'] > 20:
            explanations.append(f"üéØ –ò–∑–±—ã—Ç–æ—á–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—á–Ω–æ—Å—Ç—å ({metrics['certainty_score']}%)")
            if details['certainty_words']:
                words = details['certainty_words'][:3]
                explanations.append(f"   –°–ª–æ–≤–∞: {', '.join(words)}")
        
        if metrics['conspiracy_score'] > 20:
            explanations.append(f"üïµÔ∏è –ü—Ä–∏–∑–Ω–∞–∫–∏ —Ç–µ–æ—Ä–∏–∏ –∑–∞–≥–æ–≤–æ—Ä–∞ ({metrics['conspiracy_score']}%)")
            if details['conspiracy_words']:
                words = details['conspiracy_words'][:3]
                explanations.append(f"   –°–ª–æ–≤–∞: {', '.join(words)}")
        
        if metrics['pseudo_science_score'] > 10:
            explanations.append(f"üî¨ –ü—Å–µ–≤–¥–æ–Ω–∞—É—á–Ω—ã–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è ({metrics['pseudo_science_score']}%)")
            if details['pseudo_science_phrases']:
                phrases = details['pseudo_science_phrases'][:2]
                explanations.append(f"   –§—Ä–∞–∑—ã: {', '.join(phrases)}")
        
        if metrics['fake_stat_score'] > 10:
            explanations.append(f"üìà –°–æ–º–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ({metrics['fake_stat_score']}%)")
        
        if metrics['anonymous_score'] > 20:
            explanations.append(f"üë§ –ê–Ω–æ–Ω–∏–º–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ ({metrics['anonymous_score']}%)")
            explanations.append("   –ù–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∏–º–µ–Ω –∏ –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π")
        
        if metrics['exclamation_density'] > 20:
            explanations.append(f"‚ùó –ú–Ω–æ–≥–æ –≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏–π ({details['exclamation_count']} —à—Ç.)")
        
        if metrics['caps_ratio'] > 10:
            explanations.append(f"üî† –ú–Ω–æ–≥–æ –∑–∞–≥–ª–∞–≤–Ω—ã—Ö –±—É–∫–≤ ({metrics['caps_ratio']}%)")
        
        if metrics['source_score'] > 30:
            explanations.append(f"‚úÖ –£–ø–æ–º–∏–Ω–∞—é—Ç—Å—è –∏—Å—Ç–æ—á–Ω–∏–∫–∏ ({metrics['source_score']}%)")
        else:
            explanations.append("‚ùå –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã –∏–ª–∏ —Ä–∞—Å–ø–ª—ã–≤—á–∞—Ç—ã")
        
        if metrics['formality_score'] > 30:
            explanations.append(f"üìù –§–æ—Ä–º–∞–ª—å–Ω—ã–π —Å—Ç–∏–ª—å ({metrics['formality_score']}%)")
        
        if details['has_percentages']:
            if metrics['fake_stat_score'] > 20:
                explanations.append("‚ö†Ô∏è –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—ã–≥–ª—è–¥–∏—Ç –º–∞–Ω–∏–ø—É–ª—è—Ç–∏–≤–Ω–æ–π")
            else:
                explanations.append("üìä –ï—Å—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ")
        
        if metrics['news_source_score'] > 20:
            explanations.append("üèõÔ∏è –£–ø–æ–º–∏–Ω–∞—é—Ç—Å—è –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏")
        
        if not explanations:
            explanations.append("‚úÖ –¢–µ–∫—Å—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —è–≤–Ω—ã—Ö —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –º–∞—Ä–∫–µ—Ä–æ–≤ —Ñ–µ–π–∫–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        
        return explanations

detector = FakeNewsDetector()

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/analyze', methods=['POST'])
def analyze_text():
    try:
        data = request.get_json()
        text = data.get('text', '').strip()
        
        if not text:
            return jsonify({'error': '–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞'})
        
        if len(text) < 20:
            return jsonify({'error': '–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π (–º–∏–Ω–∏–º—É–º 20 —Å–∏–º–≤–æ–ª–æ–≤)'})
        
        if len(text) > 5000:
            return jsonify({'error': '–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π (–º–∞–∫—Å–∏–º—É–º 5000 —Å–∏–º–≤–æ–ª–æ–≤)'})
        
        analysis = detector.analyze_text(text)
        highlighted_text = detector.highlight_text(text)
        explanations = detector.generate_explanations(analysis)
        
        result = {
            'success': True,
            'reliability_score': analysis['reliability_score'],
            'fake_score': analysis['fake_score'],
            'is_fake': analysis['is_fake'],
            'verdict': analysis['verdict'],
            'highlighted_text': highlighted_text,
            'explanations': explanations,
            'metrics': analysis['metrics'],
            'details': analysis['details']
        }
        
        return jsonify(result)
        
    except Exception as e:
        app.logger.error(f"Error in analyze_text: {str(e)}")
        return jsonify({'error': f'–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞: {str(e)}'})

@app.route('/health')
def health_check():
    return jsonify({
        'status': 'ok',
        'version': '2.0',
        'algorithm': 'Rule-based —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ + –∫–æ–Ω—Å–ø–∏—Ä–æ–ª–æ–≥–∏—è',
        'python_version': '3.9.0'
    })

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port, debug=False)
