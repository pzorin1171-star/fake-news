# app.py - –ü–æ–ª–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è GitHub + Render —Å —É–ø—Ä–æ—â–µ–Ω–Ω–æ–π ML-–º–æ–¥–µ–ª—å—é
from flask import Flask, render_template, request, jsonify
import re
import os
import pickle
import numpy as np
from textblob import TextBlob

app = Flask(__name__)

# ============================================================================
# 1. RULE-BASED –î–ï–¢–ï–ö–¢–û–† (–í–∞—à —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ —Å —É–ª—É—á—à–µ–Ω–∏—è–º–∏)
# ============================================================================
class FakeNewsDetector:
    def __init__(self):
        # –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞—Ä–∏
        self.clickbait_words = ['—à–æ–∫', '—Å–µ–Ω—Å–∞—Ü–∏—è', '—Ç–∞–π–Ω–∞', '—Å–∫–∞–Ω–¥–∞–ª', '—Ä–∞–∑–æ–±–ª–∞—á–µ–Ω–∏–µ',
                               '—É–∂–∞—Å', '—á—É–¥–æ', '–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ', '–ø–æ—Ç—Ä—è—Å–∞—é—â–µ', '—Å—Ä–æ—á–Ω–æ',
                               '—ç–∫—Å–∫–ª—é–∑–∏–≤', '—Å–µ–∫—Ä–µ—Ç', '–ø—Ä–∞–≤–¥–∞', '–ª–æ–∂—å', '–æ–±–º–∞–Ω',
                               '—à–æ–∫–∏—Ä—É—é—â–µ–µ', '–∂—É—Ç–∫–∏–π', '–∞–¥—Å–∫–∏–π', '—á—É–¥–æ–≤–∏—â–Ω—ã–π', '–≥–Ω—É—Å–Ω—ã–π',
                               '—à–æ–∫–∏—Ä—É—é—â–µ–µ –æ—Ç–∫—Ä—ã—Ç–∏–µ', '—Ü–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ', '–∞–¥—Å–∫–∏–π –∑–∞–≥–æ–≤–æ—Ä',
                               '–∂—É—Ç–∫–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞', '–∑–∞–º–∞–ª—á–∏–≤–∞–ª–∏', '—Å—Ä–æ—á–Ω–æ', '—ç–∫—Å–∫–ª—é–∑–∏–≤–Ω–æ']

        self.certainty_words = ['—Ç–æ—á–Ω–æ', '–∞–±—Å–æ–ª—é—Ç–Ω–æ', '–Ω–µ—Å–æ–º–Ω–µ–Ω–Ω–æ', '–±–µ–∑—É—Å–ª–æ–≤–Ω–æ',
                               '–∫–æ–Ω–µ—á–Ω–æ', '—è–≤–Ω–æ', '–æ—á–µ–≤–∏–¥–Ω–æ', '–Ω–∞–≤–µ—Ä–Ω—è–∫–∞',
                               '–≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ', '—Å—Ç–æ–ø—Ä–æ—Ü–µ–Ω—Ç–Ω–æ', '–¥–æ–∫–∞–∑–∞–Ω–æ', '–ø–æ–ª–Ω–æ—Å—Ç—å—é']

        self.formal_words = ['—Å–æ–æ–±—â–∏–ª', '–∑–∞—è–≤–∏–ª', '–æ—Ç–º–µ—Ç–∏–ª', '–ø–æ–¥—á–µ—Ä–∫–Ω—É–ª',
                            '—É–∫–∞–∑–∞–ª', '–¥–æ–±–∞–≤–∏–ª', '–ø–æ –¥–∞–Ω–Ω—ã–º', '—Å–æ–≥–ª–∞—Å–Ω–æ', '–∑–∞–º–µ—Ç–∏–ª']

        self.source_indicators = ['–ø–æ –¥–∞–Ω–Ω—ã–º', '—Å–æ–≥–ª–∞—Å–Ω–æ', '–∫–∞–∫ —Å–æ–æ–±—â–∞–µ—Ç',
                                 '–ø–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏', '–ø–æ —Å–ª–æ–≤–∞–º', '–ø–æ —Å–≤–µ–¥–µ–Ω–∏—è–º',
                                 '–∏—Å—Ç–æ—á–Ω–∏–∫', '—ç–∫—Å–ø–µ—Ä—Ç', '—É—á–µ–Ω—ã–π', '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ']

        self.news_sources = ['—Ü–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '–º–∏–Ω–∑–¥—Ä–∞–≤', '—Ä–æ—Å–ø–æ—Ç—Ä–µ–±–Ω–∞–¥–∑–æ—Ä',
                            '—Ä–æ—Å—Å—Ç–∞—Ç', '–æ–æ–Ω', '–≤—Å–µ–º–∏—Ä–Ω—ã–π –±–∞–Ω–∫', '–º–≤—Ñ', '—ç–∫—Å–ø–µ—Ä—Ç—ã', '–∞–Ω–∞–ª–∏—Ç–∏–∫–∏',
                            '—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç', '–∏–Ω—Å—Ç–∏—Ç—É—Ç', '–ª–∞–±–æ—Ä–∞—Ç–æ—Ä–∏—è', '–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏']

        # –ù–æ–≤—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        self.conspiracy_words = ['–∑–∞–≥–æ–≤–æ—Ä', '–≥–ª–æ–±–∞–ª–∏—Å—Ç—ã', '–º–∏—Ä–æ–≤–æ–µ –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ',
                                '–º–∞—Ñ–∏—è', '—Ç–∞–π–Ω—ã–π –∞–ª—å—è–Ω—Å', '—Å–∏–ª—å–Ω—ã–µ –º–∏—Ä–∞ —Å–µ–≥–æ',
                                '—Å–∏—Å—Ç–µ–º–∞', '–∞–≥–µ–Ω—Ç—ã', '–∫—É–ø–ª–µ–Ω—ã', '—Å–∫—Ä—ã–≤–∞—é—Ç',
                                '–∑–∞–º–∞–ª—á–∏–≤–∞—é—Ç', '—Å–æ–∫—Ä—ã—Ç–∏–µ', '–ø—Ä–∞–≤–¥—É —Å–∫—Ä—ã–≤–∞—é—Ç',
                                '–º–µ–≥–∞–∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–π –∑–∞–≥–æ–≤–æ—Ä', '–∫—Ä–µ–º–Ω–∏–µ–≤–∞—è –º–∞—Ñ–∏—è',
                                '—Ñ–∞—Ä–º–∞—Ü–µ–≤—Ç–∏—á–µ—Å–∫–∏–µ –≥–∏–≥–∞–Ω—Ç—ã', '–∞–≥–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã']

        self.pseudo_science = ['—Ç–æ–∫—Å–∏–Ω—ã –º—ã—à–ª–µ–Ω–∏—è', '–Ω–µ–π—Ä–æ–Ω—ã –≤—ã–∂–∏–≥–∞–µ—Ç', '–∏–∑–ª—É—á–µ–Ω–∏–µ',
                              '–≤–æ–ª–Ω—ã', '–ø—Ä–æ–≥—Ä–∞–º–º–∞ —É–Ω–∏—á—Ç–æ–∂–µ–Ω–∏—è', '–¥–µ–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π',
                              '–æ—Ä—É–∂–µ–Ω–∏–µ –º–∞—Å—Å–æ–≤–æ–≥–æ', '–Ω–µ–π—Ä–æ-—â–∏—Ç', '–±–ª–æ–∫–∏—Ä—É–µ—Ç',
                              '—Ç–æ–∫—Å–∏–Ω—ã –º—ã—à–ª–µ–Ω–∏—è', '–Ω–µ–π—Ä–æ–Ω—ã –≤—ã–∂–∏–≥–∞–µ—Ç', '–Ω–µ–π—Ä–æ-—â–∏—Ç',
                              '–±–ª–æ–∫–∏—Ä—É–µ—Ç 99%', '–≤–æ–ª–Ω—ã –±—É–∫–≤–∞–ª—å–Ω–æ –≤—ã–∂–∏–≥–∞—é—Ç',
                              '–∏–∑–ª—É—á–µ–Ω–∏–µ —Å–º–∞—Ä—Ç—Ñ–æ–Ω–æ–≤', '–ø—Ä–æ–≥—Ä–∞–º–º–∞ —É–Ω–∏—á—Ç–æ–∂–µ–Ω–∏—è']

        self.fake_stat_words = ['–Ω–∞ 300% –≤—ã—à–µ', '–Ω–∞ 47% —Å–Ω–∏–∂–∞–µ—Ç—Å—è', '99% —Ç–æ–∫—Å–∏–Ω–æ–≤',
                               '–¥–æ–∫–∞–∑–∞–Ω–æ —Ñ–∞–∫—Ç–∞–º–∏', '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ—Ç–æ—Ä—É—é —Å–∫—Ä—ã–≤–∞—é—Ç',
                               '–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω—ã–µ –Ω–æ –¥–æ–∫–∞–∑–∞–Ω–Ω—ã–µ']

        self.anonymous_sources = ['–ø—Å–µ–≤–¥–æ–Ω–∏–º', '–∏–º—è –∏–∑–º–µ–Ω–µ–Ω–æ', '–ø–æ–∂–µ–ª–∞–≤—à–∏–π –æ—Å—Ç–∞—Ç—å—Å—è –∞–Ω–æ–Ω–∏–º–Ω—ã–º',
                                 '–Ω–∞—à –∏—Å—Ç–æ—á–Ω–∏–∫', '–Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —ç–∫—Å–ø–µ—Ä—Ç—ã', '—É—á–µ–Ω—ã–π –∫–æ—Ç–æ—Ä—ã–π',
                                 '–¥–æ–∫—Ç–æ—Ä', '—ç–∫—Å–ø–µ—Ä—Ç –ø–æ–¥ –ø—Ä–∏–∫—Ä—ã—Ç–∏–µ–º']

        self.emotional_manipulation = ['—Å–ø–∞—Å–∏—Ç–µ', '–ø–æ–∫–∞ –Ω–µ –ø–æ–∑–¥–Ω–æ', '–ø—Ä–æ—Å–Ω–∏—Ç–µ—Å—å', '–±–æ—Ä—å–±–∞ –Ω–∞—á–∞–ª–∞—Å—å',
                                       '–Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ', '—Ç—Ä–µ–±—É–π—Ç–µ', '—Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–∏—Ç–µ', '–≤—ã–∫–∏–Ω—å—Ç–µ', '–∑–∞–º–µ–Ω–∏—Ç–µ',
                                       '–∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ', '–ø–æ–ª–Ω–æ–µ —É–Ω–∏—á—Ç–æ–∂–µ–Ω–∏–µ', '–±–µ–∑–Ω–∞–¥—ë–∂–Ω—ã–º–∏', '–≥–ª—É–ø—ã–º–∏']

    def analyze_text(self, text):
        text_lower = text.lower()
        words = re.findall(r'\b\w+\b', text_lower)
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]

        # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        clickbait_count = sum(1 for word in self.clickbait_words if word in text_lower)
        clickbait_score = min(clickbait_count / 2, 1.0)

        # –ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ TextBlob
        try:
            blob = TextBlob(text)
            emotional_score = abs(blob.sentiment.polarity)
            if emotional_score > 0.5:
                emotional_score = min(emotional_score * 1.5, 1.0)
        except:
            emotional_score = 0

        certainty_count = sum(1 for word in self.certainty_words if word in text_lower)
        certainty_score = min(certainty_count / 2, 1.0)

        formal_count = sum(1 for word in self.formal_words if word in text_lower)
        formality_score = min(formal_count / 3, 1.0)

        source_count = sum(1 for word in self.source_indicators if word in text_lower)
        source_score = min(source_count / 2, 1.0)

        news_source_count = sum(1 for source in self.news_sources if source in text_lower)
        news_source_score = min(news_source_count / 2, 1.0)

        exclamation_count = text.count('!')
        exclamation_density = exclamation_count / max(len(sentences), 1)
        if exclamation_density > 0.5:
            exclamation_density = 1.0

        caps_count = sum(1 for c in text if c.isupper())
        caps_ratio = caps_count / max(len(text), 1)
        if caps_ratio > 0.1:
            caps_ratio = min(caps_ratio * 2, 1.0)

        # –ù–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        conspiracy_count = sum(1 for word in self.conspiracy_words if word in text_lower)
        conspiracy_score = min(conspiracy_count / 2, 1.0)

        pseudo_science_count = 0
        for phrase in self.pseudo_science:
            if phrase in text_lower:
                pseudo_science_count += 1
        pseudo_science_score = min(pseudo_science_count / 1, 1.0)

        fake_stat_count = sum(1 for word in self.fake_stat_words if word in text_lower)
        fake_stat_score = min(fake_stat_count / 1, 1.0)

        anonymous_count = sum(1 for word in self.anonymous_sources if word in text_lower)
        anonymous_score = min(anonymous_count / 1, 1.0)

        action_urgency_count = sum(1 for word in self.emotional_manipulation if word in text_lower)
        urgency_score = min(action_urgency_count / 2, 1.0)

        vague_source_penalty = 0
        if ('–¥–æ–∫—Ç–æ—Ä' in text_lower or '—É—á–µ–Ω—ã–π' in text_lower or '–∏—Å—Ç–æ—á–Ω–∏–∫' in text_lower) and \
           any(word in text_lower for word in ['–¥–æ–∫–∞–∑–∞–Ω–æ', '—Ç–æ—á–Ω–æ', '–∞–±—Å–æ–ª—é—Ç–Ω–æ']):
            vague_source_penalty = 0.3

        only_exclam_quest = all(any(c in s for c in '!?') for s in sentences if s)
        structure_penalty = 0.4 if only_exclam_quest and len(sentences) > 3 else 0

        # –†–∞—Å—á–µ—Ç –æ–±—â–µ–≥–æ –±–∞–ª–ª–∞ —Ñ–µ–π–∫–æ–≤–æ—Å—Ç–∏
        fake_score = (
            clickbait_score * 0.15 +
            emotional_score * 0.20 +
            certainty_score * 0.10 +
            exclamation_density * 0.15 +
            caps_ratio * 0.05 +
            (1 - formality_score) * 0.03 +
            conspiracy_score * 0.12 +
            pseudo_science_score * 0.15 +
            fake_stat_score * 0.10 +
            anonymous_score * 0.08 +
            urgency_score * 0.10 +
            vague_source_penalty +
            structure_penalty
        )

        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏
        fake_score = min(max(fake_score, 0), 1)

        if news_source_score > 0.3 and anonymous_score < 0.3:
            fake_score *= 0.7
        elif anonymous_score > 0.3:
            fake_score *= 1.2

        # –î–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –∏ –≤–µ—Ä–¥–∏–∫—Ç
        reliability_score = max(0, min(100, round(100 - (fake_score * 100))))

        if reliability_score >= 80:
            verdict = "–í–´–°–û–ö–ê–Ø –î–û–°–¢–û–í–ï–†–ù–û–°–¢–¨"
            is_fake = False
        elif reliability_score >= 60:
            verdict = "–°–†–ï–î–ù–Ø–Ø –î–û–°–¢–û–í–ï–†–ù–û–°–¢–¨"
            is_fake = fake_score > 0.6
        else:
            verdict = "–ù–ò–ó–ö–ê–Ø –î–û–°–¢–û–í–ï–†–ù–û–°–¢–¨"
            is_fake = True

        # –§–æ—Ä–º–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        metrics = {
            'clickbait_score': round(clickbait_score * 100),
            'emotional_score': round(emotional_score * 100),
            'certainty_score': round(certainty_score * 100),
            'formality_score': round(formality_score * 100),
            'source_score': round(source_score * 100),
            'news_source_score': round(news_source_score * 100),
            'exclamation_density': round(exclamation_density * 100),
            'caps_ratio': round(caps_ratio * 100),
            'conspiracy_score': round(conspiracy_score * 100),
            'pseudo_science_score': round(pseudo_science_score * 100),
            'fake_stat_score': round(fake_stat_score * 100),
            'anonymous_score': round(anonymous_score * 100),
            'urgency_score': round(urgency_score * 100)
        }

        # –í–µ–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –≤–æ–∑–º–æ–∂–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
        ml_feature_vector = [
            metrics['clickbait_score'] / 100,
            metrics['emotional_score'] / 100,
            metrics['conspiracy_score'] / 100,
            metrics['pseudo_science_score'] / 100,
            metrics['exclamation_density'] / 100,
            metrics['caps_ratio'] / 100,
            metrics['certainty_score'] / 100,
            metrics['anonymous_score'] / 100,
            fake_score,
            min(len(text) / 1000, 1.0)
        ]

        return {
            'reliability_score': reliability_score,
            'fake_score': round(fake_score * 100, 1),
            'is_fake': is_fake,
            'verdict': verdict,
            'metrics': metrics,
            'ml_feature_vector': ml_feature_vector,
            'details': {
                'clickbait_words': list(set([w for w in self.clickbait_words if w in text_lower])),
                'certainty_words': list(set([w for w in self.certainty_words if w in text_lower])),
                'conspiracy_words': list(set([w for w in self.conspiracy_words if w in text_lower])),
                'pseudo_science_phrases': list(set([p for p in self.pseudo_science if p in text_lower])),
                'exclamation_count': exclamation_count,
                'has_percentages': bool('%' in text or '–ø—Ä–æ—Ü–µ–Ω—Ç' in text_lower),
                'word_count': len(words),
                'sentence_count': len(sentences),
                'anonymous_sources_detected': anonymous_count > 0
            }
        }

    def highlight_text(self, text):
        """–ü–æ–¥—Å–≤–µ—Ç–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        highlighted = text
        
        # –ü–æ–¥—Å–≤–µ—Ç–∫–∞ –∫–ª–∏–∫–±–µ–π—Ç–∞
        for word in self.clickbait_words:
            pattern = re.compile(re.escape(word), re.IGNORECASE)
            highlighted = pattern.sub(
                f'<span class="highlight clickbait">{word.upper()}</span>',
                highlighted
            )
        
        # –ü–æ–¥—Å–≤–µ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—á–Ω–æ—Å—Ç–∏
        for word in self.certainty_words:
            pattern = re.compile(re.escape(word), re.IGNORECASE)
            highlighted = pattern.sub(
                f'<span class="highlight certainty">{word.upper()}</span>',
                highlighted
            )
        
        # –ü–æ–¥—Å–≤–µ—Ç–∫–∞ –∫–æ–Ω—Å–ø–∏—Ä–æ–ª–æ–≥–∏–∏
        for word in self.conspiracy_words:
            pattern = re.compile(re.escape(word), re.IGNORECASE)
            highlighted = pattern.sub(
                f'<span class="highlight conspiracy">{word.upper()}</span>',
                highlighted
            )
        
        return highlighted

    def generate_explanations(self, analysis):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ—è—Å–Ω–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        explanations = []
        metrics = analysis['metrics']
        details = analysis['details']
        
        if metrics['clickbait_score'] > 20:
            explanations.append(f"‚ö†Ô∏è –í—ã—Å–æ–∫–∏–π –∫–ª–∏–∫–±–µ–π—Ç-–∏–Ω–¥–µ–∫—Å ({metrics['clickbait_score']}%)")
        
        if metrics['emotional_score'] > 30:
            explanations.append(f"üò† –í—ã—Å–æ–∫–∞—è —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å ({metrics['emotional_score']}%)")
        
        if metrics['conspiracy_score'] > 20:
            explanations.append(f"üïµÔ∏è –ü—Ä–∏–∑–Ω–∞–∫–∏ —Ç–µ–æ—Ä–∏–∏ –∑–∞–≥–æ–≤–æ—Ä–∞ ({metrics['conspiracy_score']}%)")
        
        if metrics['pseudo_science_score'] > 10:
            explanations.append(f"üî¨ –ü—Å–µ–≤–¥–æ–Ω–∞—É—á–Ω—ã–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è ({metrics['pseudo_science_score']}%)")
        
        if metrics['exclamation_density'] > 20:
            explanations.append(f"‚ùó –ú–Ω–æ–≥–æ –≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏–π ({details['exclamation_count']} —à—Ç.)")
        
        if metrics['caps_ratio'] > 10:
            explanations.append(f"üî† –ú–Ω–æ–≥–æ –∑–∞–≥–ª–∞–≤–Ω—ã—Ö –±—É–∫–≤ ({metrics['caps_ratio']}%)")
        
        if not explanations:
            explanations.append("‚úÖ –¢–µ–∫—Å—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —è–≤–Ω—ã—Ö —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –º–∞—Ä–∫–µ—Ä–æ–≤ —Ñ–µ–π–∫–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        
        return explanations

# ============================================================================
# 2. ML –ú–û–î–ï–õ–¨ (–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è Render)
# ============================================================================
class SimpleMLPredictor:
    def __init__(self, model_path='simple_model.pkl'):
        self.model = None
        self.model_loaded = False
        
        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å
        if os.path.exists(model_path):
            try:
                with open(model_path, 'rb') as f:
                    self.model = pickle.load(f)
                self.model_loaded = True
                print(f"[ML] –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {model_path}")
            except Exception as e:
                print(f"[ML] –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        else:
            print(f"[ML] –§–∞–π–ª –º–æ–¥–µ–ª–∏ {model_path} –Ω–µ –Ω–∞–π–¥–µ–Ω. ML –∞–Ω–∞–ª–∏–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")
    
    def predict(self, text):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ML –º–æ–¥–µ–ª–∏"""
        if not self.model_loaded or self.model is None:
            return {
                'is_fake': None,
                'confidence': 0,
                'error': 'ML –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞',
                'available': False
            }
        
        try:
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
            prediction = self.model.predict_proba([text])[0]
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–ª–∞—Å—Å (1 - —Ñ–µ–π–∫, 0 - –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ)
            is_fake = self.model.predict([text])[0] == 1
            
            # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏
            confidence = prediction[1] if is_fake else prediction[0]
            
            return {
                'is_fake': bool(is_fake),
                'confidence': round(float(confidence * 100), 2),
                'ml_score': round(float(prediction[1]), 4),
                'verdict': '–§–ï–ô–ö (ML –∞–Ω–∞–ª–∏–∑)' if is_fake else '–î–û–°–¢–û–í–ï–†–ù–û (ML –∞–Ω–∞–ª–∏–∑)',
                'available': True
            }
        except Exception as e:
            return {
                'is_fake': None,
                'confidence': 0,
                'error': f'–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {str(e)}',
                'available': False
            }
    
    def is_loaded(self):
        return self.model_loaded

# ============================================================================
# 3. –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ö–û–ú–ü–û–ù–ï–ù–¢–û–í
# ============================================================================
print("=" * 60)
print("FAKE NEWS DETECTOR - Simplified Version for Render")
print("=" * 60)

# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä—ã –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤
rule_detector = FakeNewsDetector()
ml_predictor = SimpleMLPredictor()

print(f"[System] Rule-based –¥–µ—Ç–µ–∫—Ç–æ—Ä: ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω")
print(f"[System] ML –º–æ–¥–µ–ª—å: {'‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞' if ml_predictor.is_loaded() else '‚ö†Ô∏è –ù–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞'}")
print("=" * 60)

# ============================================================================
# 4. FLASK ROUTES (–ú–∞—Ä—à—Ä—É—Ç—ã –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è)
# ============================================================================
@app.route('/')
def index():
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞"""
    ml_status = "‚úÖ –ê–∫—Ç–∏–≤–Ω–∞" if ml_predictor.is_loaded() else "‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç—É–ø–Ω–∞"
    return render_template('index.html', 
                         ml_status=ml_status,
                         title="–î–µ—Ç–µ–∫—Ç–æ—Ä —Ñ–µ–π–∫–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")

@app.route('/analyze', methods=['POST'])
def analyze_rule_based():
    """–¢–æ–ª—å–∫–æ rule-based –∞–Ω–∞–ª–∏–∑"""
    try:
        data = request.get_json()
        text = data.get('text', '').strip()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if not text:
            return jsonify({'error': '–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞', 'success': False})
        
        if len(text) < 20:
            return jsonify({'error': '–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π (–º–∏–Ω–∏–º—É–º 20 —Å–∏–º–≤–æ–ª–æ–≤)', 'success': False})
        
        if len(text) > 5000:
            return jsonify({'error': '–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π (–º–∞–∫—Å–∏–º—É–º 5000 —Å–∏–º–≤–æ–ª–æ–≤)', 'success': False})
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑
        analysis = rule_detector.analyze_text(text)
        highlighted = rule_detector.highlight_text(text)
        explanations = rule_detector.generate_explanations(analysis)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        response = {
            'success': True,
            'type': 'rule_based',
            'reliability_score': analysis['reliability_score'],
            'fake_score': analysis['fake_score'],
            'is_fake': analysis['is_fake'],
            'verdict': analysis['verdict'],
            'highlighted_text': highlighted,
            'explanations': explanations,
            'metrics': analysis['metrics'],
            'details': analysis['details'],
            'text_length': len(text)
        }
        
        return jsonify(response)
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞: {str(e)}'
        })

@app.route('/analyze_hybrid', methods=['POST'])
def analyze_hybrid():
    """–ì–∏–±—Ä–∏–¥–Ω—ã–π –∞–Ω–∞–ª–∏–∑: rule-based + ML"""
    try:
        data = request.get_json()
        text = data.get('text', '').strip()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if not text:
            return jsonify({'error': '–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞', 'success': False})
        
        if len(text) < 20:
            return jsonify({'error': '–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π (–º–∏–Ω–∏–º—É–º 20 —Å–∏–º–≤–æ–ª–æ–≤)', 'success': False})
        
        if len(text) > 5000:
            return jsonify({'error': '–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π (–º–∞–∫—Å–∏–º—É–º 5000 —Å–∏–º–≤–æ–ª–æ–≤)', 'success': False})
        
        # Rule-based –∞–Ω–∞–ª–∏–∑
        rule_analysis = rule_detector.analyze_text(text)
        highlighted = rule_detector.highlight_text(text)
        explanations = rule_detector.generate_explanations(rule_analysis)
        
        # ML –∞–Ω–∞–ª–∏–∑
        ml_result = ml_predictor.predict(text)
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if ml_result['available']:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º ML –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            final_verdict = ml_result['verdict']
            final_is_fake = ml_result['is_fake']
            final_confidence = ml_result['confidence']
            ml_available = True
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º rule-based –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            final_verdict = rule_analysis['verdict']
            final_is_fake = rule_analysis['is_fake']
            final_confidence = rule_analysis['reliability_score']
            ml_available = False
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        response = {
            'success': True,
            'type': 'hybrid',
            'ml_available': ml_available,
            
            # –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            'is_fake': final_is_fake,
            'verdict': final_verdict,
            'confidence': final_confidence,
            
            # Rule-based –¥–µ—Ç–∞–ª–∏
            'rule_based_score': rule_analysis['reliability_score'],
            'highlighted_text': highlighted,
            'explanations': explanations,
            'details': rule_analysis['details'],
            
            # ML –¥–µ—Ç–∞–ª–∏ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
            'ml_result': ml_result if ml_available else None,
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            'text_length': len(text),
            'processing_time': '–º–≥–Ω–æ–≤–µ–Ω–Ω–æ'
        }
        
        return jsonify(response)
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞: {str(e)}'
        })

@app.route('/health', methods=['GET'])
def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞"""
    return jsonify({
        'status': 'healthy',
        'service': 'Fake News Detector API',
        'version': '2.0-simple',
        'timestamp': os.times().user,
        'components': {
            'rule_based_detector': 'active',
            'ml_model': 'loaded' if ml_predictor.is_loaded() else 'not_loaded',
            'ml_type': 'LogisticRegression + TF-IDF'
        },
        'endpoints': {
            'home': '/',
            'rule_based_analysis': '/analyze (POST)',
            'hybrid_analysis': '/analyze_hybrid (POST)',
            'health_check': '/health (GET)'
        }
    })

@app.route('/api/status', methods=['GET'])
def api_status():
    """–°—Ç–∞—Ç—É—Å API"""
    return jsonify({
        'ml_model_loaded': ml_predictor.is_loaded(),
        'rule_based_active': True,
        'total_endpoints': 4,
        'max_text_length': 5000,
        'min_text_length': 20
    })

# ============================================================================
# 5. –ó–ê–ü–£–°–ö –°–ï–†–í–ï–†–ê
# ============================================================================
if __name__ == '__main__':
    # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Ä—Ç –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è (–¥–ª—è Render)
    port = int(os.environ.get('PORT', 5000))
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–µ—Ä
    print(f"\n[Server] –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞ –Ω–∞ –ø–æ—Ä—Ç—É {port}")
    print(f"[Server] –î–æ—Å—Ç—É–ø–Ω–æ –ø–æ –∞–¥—Ä–µ—Å—É: http://localhost:{port}")
    print(f"[Server] Health check: http://localhost:{port}/health")
    print("=" * 60)
    
    app.run(host='0.0.0.0', port=port, debug=False)
