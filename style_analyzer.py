import re
import pandas as pd
import numpy as np
from textblob import TextBlob
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import joblib
import os
import logging

logger = logging.getLogger(__name__)

class StyleAnalyzer:
    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        self.model_accuracy = 0.85
        
        # –°–ø–∏—Å–∫–∏ —Å–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        self.clickbait_words = ['—à–æ–∫', '—Å–µ–Ω—Å–∞—Ü–∏—è', '—Ç–∞–π–Ω–∞', '—Å–∫–∞–Ω–¥–∞–ª', '—Ä–∞–∑–æ–±–ª–∞—á–µ–Ω–∏–µ', 
                               '—É–∂–∞—Å', '—á—É–¥–æ', '–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ', '–ø–æ—Ç—Ä—è—Å–∞—é—â–µ', '—Å—Ä–æ—á–Ω–æ',
                               '—ç–∫—Å–∫–ª—é–∑–∏–≤', '—Å–µ–∫—Ä–µ—Ç', '–ø—Ä–∞–≤–¥–∞', '–ª–æ–∂—å', '–æ–±–º–∞–Ω']
        
        self.certainty_words = ['—Ç–æ—á–Ω–æ', '–∞–±—Å–æ–ª—é—Ç–Ω–æ', '–Ω–µ—Å–æ–º–Ω–µ–Ω–Ω–æ', '–±–µ–∑—É—Å–ª–æ–≤–Ω–æ',
                               '–∫–æ–Ω–µ—á–Ω–æ', '—è–≤–Ω–æ', '–æ—á–µ–≤–∏–¥–Ω–æ', '–Ω–∞–≤–µ—Ä–Ω—è–∫–∞',
                               '–≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ', '—Å—Ç–æ–ø—Ä–æ—Ü–µ–Ω—Ç–Ω–æ', '–¥–æ–∫–∞–∑–∞–Ω–æ']
        
        self.formal_words = ['—Å–æ–æ–±—â–∏–ª', '–∑–∞—è–≤–∏–ª', '–æ—Ç–º–µ—Ç–∏–ª', '–ø–æ–¥—á–µ—Ä–∫–Ω—É–ª',
                            '—É–∫–∞–∑–∞–ª', '–¥–æ–±–∞–≤–∏–ª', '–ø–æ –¥–∞–Ω–Ω—ã–º', '—Å–æ–≥–ª–∞—Å–Ω–æ']
        
        self.source_indicators = ['–ø–æ –¥–∞–Ω–Ω—ã–º', '—Å–æ–≥–ª–∞—Å–Ω–æ', '–∫–∞–∫ —Å–æ–æ–±—â–∞–µ—Ç',
                                 '–ø–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏', '–ø–æ —Å–ª–æ–≤–∞–º', '–ø–æ —Å–≤–µ–¥–µ–Ω–∏—è–º']
    
    def extract_features(self, text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        features = {}
        text_lower = text.lower()
        
        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        features['length'] = len(text)
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        words = re.findall(r'\b\w+\b', text_lower)
        
        features['sentence_count'] = len(sentences)
        features['word_count'] = len(words)
        
        if sentences:
            features['avg_words_per_sentence'] = len(words) / len(sentences)
        else:
            features['avg_words_per_sentence'] = 0
        
        # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
        try:
            blob = TextBlob(text)
            features['emotional_score'] = abs(blob.sentiment.polarity)
            features['subjectivity'] = blob.sentiment.subjectivity
        except:
            features['emotional_score'] = 0
            features['subjectivity'] = 0
        
        # –ö–ª–∏–∫–±–µ–π—Ç
        clickbait_count = sum(1 for word in self.clickbait_words if word in text_lower)
        features['clickbait_score'] = min(clickbait_count / 3, 1.0)
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏—á–Ω–æ—Å—Ç—å
        certainty_count = sum(1 for word in self.certainty_words if word in text_lower)
        features['certainty_score'] = min(certainty_count / 3, 1.0)
        
        # –§–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—å
        formal_count = sum(1 for word in self.formal_words if word in text_lower)
        features['formality_score'] = min(formal_count / 3, 1.0)
        
        # –ò—Å—Ç–æ—á–Ω–∏–∫–∏
        source_count = sum(1 for word in self.source_indicators if word in text_lower)
        features['source_indicator_score'] = min(source_count / 2, 1.0)
        
        # –ë–∞–ª–∞–Ω—Å
        balance_words = ['–æ–¥–Ω–∞–∫–æ', '—Ç–µ–º –Ω–µ –º–µ–Ω–µ–µ', '—Å –¥—Ä—É–≥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã']
        balance_count = sum(1 for word in balance_words if word in text_lower)
        features['balance_score'] = min(balance_count / 2, 1.0)
        
        # –ü—É–Ω–∫—Ç—É–∞—Ü–∏—è
        features['exclamation_count'] = text.count('!')
        features['exclamation_density'] = features['exclamation_count'] / max(len(sentences), 1)
        
        # –†–µ–≥–∏—Å—Ç—Ä
        caps_count = sum(1 for c in text if c.isupper())
        features['caps_ratio'] = caps_count / len(text) if len(text) > 0 else 0
        
        # –ß–∏—Å–ª–∞ –∏ –ø—Ä–æ—Ü–µ–Ω—Ç—ã
        features['has_percentages'] = 1 if '%' in text or '–ø—Ä–æ—Ü–µ–Ω—Ç' in text_lower else 0
        
        return features
    
    def generate_synthetic_data(self, n_samples=600):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        data = []
        
        fake_templates = [
            "–®–û–ö! {subject} —Å–∫—Ä—ã–≤–∞—é—Ç –ü–†–ê–í–î–£ –æ {topic}! –í—Å–µ –≤ –£–ñ–ê–°–ï!",
            "–°–ï–ù–°–ê–¶–ò–Ø! {subject} –†–ê–ó–û–ë–õ–ê–ß–ò–õ–ò {topic}! –≠—Ç–æ –ù–ï–í–ï–†–û–Ø–¢–ù–û!",
            "–¢–ê–ô–ù–ê {topic} –†–ê–°–ö–†–´–¢–ê! {subject} –≤ –®–û–ö–ï!",
            "{subject} –û–ë–ú–ê–ù–´–í–ê–Æ–¢ –Ω–∞—Å! –ü–†–ê–í–î–ê –æ {topic}!",
            "–í–°–ö–†–´–õ–ê–°–¨ –¢–ê–ô–ù–ê! {topic} - —ç—Ç–æ –û–ü–ê–°–ù–û–°–¢–¨!"
        ]
        
        real_templates = [
            "–ü–æ –¥–∞–Ω–Ω—ã–º {source}, {topic} —Å–æ—Å—Ç–∞–≤–∏–ª {number}%",
            "–≠–∫—Å–ø–µ—Ä—Ç—ã –æ—Ç–º–µ—á–∞—é—Ç, —á—Ç–æ {topic} –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ä–æ—Å—Ç",
            "–°–æ–≥–ª–∞—Å–Ω–æ –æ—Ç—á–µ—Ç—É {source}, {topic} —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞–ª—Å—è",
            "–ê–Ω–∞–ª–∏—Ç–∏–∫–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É—é—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–µ {topic}",
            "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ {topic} —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –Ω–æ—Ä–º–µ"
        ]
        
        subjects = ["—É—á–µ–Ω—ã–µ", "–≤–ª–∞—Å—Ç–∏", "–≤—Ä–∞—á–∏", "–ø–æ–ª–∏—Ç–∏–∫–∏"]
        topics = ["–∏–Ω—Ñ–ª—è—Ü–∏–∏", "—ç–∫–æ–Ω–æ–º–∏–∫–µ", "–∫–ª–∏–º–∞—Ç–µ", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö"]
        sources = ["–¶–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫–∞", "–ú–∏–Ω–∑–¥—Ä–∞–≤–∞", "–†–æ—Å—Å—Ç–∞—Ç–∞", "—ç–∫—Å–ø–µ—Ä—Ç–æ–≤"]
        
        # –§–µ–π–∫–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
        for _ in range(n_samples // 2):
            template = np.random.choice(fake_templates)
            text = template.format(
                subject=np.random.choice(subjects),
                topic=np.random.choice(topics),
                source=np.random.choice(sources),
                number=np.random.randint(1, 99)
            )
            
            features = self.extract_features(text)
            features['text'] = text
            features['is_fake'] = 1
            data.append(features)
        
        # –ù–∞—Å—Ç–æ—è—â–∏–µ –Ω–æ–≤–æ—Å—Ç–∏
        for _ in range(n_samples // 2):
            template = np.random.choice(real_templates)
            text = template.format(
                source=np.random.choice(sources),
                topic=np.random.choice(topics),
                number=np.random.randint(1, 99)
            )
            
            features = self.extract_features(text)
            features['text'] = text
            features['is_fake'] = 0
            data.append(features)
        
        return pd.DataFrame(data)
    
    def load_or_train_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        model_path = 'models/fake_news_model.pkl'
        scaler_path = 'models/scaler.pkl'
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É models –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        if not os.path.exists('models'):
            os.makedirs('models', exist_ok=True)
        
        try:
            # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –º–æ–¥–µ–ª—å
            if os.path.exists(model_path) and os.path.exists(scaler_path):
                self.model = joblib.load(model_path)
                self.scaler = joblib.load(scaler_path)
                logger.info("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ —Ñ–∞–π–ª–∞")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                test_df = self.generate_synthetic_data(100)
                X_test, y_test = self._prepare_features(test_df)
                X_test_scaled = self.scaler.transform(X_test)
                accuracy = self.model.score(X_test_scaled, y_test)
                self.model_accuracy = accuracy
                logger.info(f"üìä –¢–æ—á–Ω–æ—Å—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏: {accuracy:.2%}")
                return
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å: {e}")
        
        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å, –æ–±—É—á–∞–µ–º –Ω–æ–≤—É—é
        logger.info("üéì –ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏...")
        self.train_model()
    
    def train_model(self):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        df = self.generate_synthetic_data(600)
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        X, y = self._prepare_features(df)
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        self.model = GradientBoostingClassifier(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=5,
            random_state=42
        )
        
        self.model.fit(X_train_scaled, y_train)
        
        # –û—Ü–µ–Ω–∫–∞
        train_accuracy = self.model.score(X_train_scaled, y_train)
        test_accuracy = self.model.score(X_test_scaled, y_test)
        self.model_accuracy = test_accuracy
        
        logger.info(f"üìä –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏: {train_accuracy:.2%}")
        logger.info(f"üìä –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ: {test_accuracy:.2%}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        joblib.dump(self.model, 'models/fake_news_model.pkl')
        joblib.dump(self.scaler, 'models/scaler.pkl')
        
        logger.info("üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
    
    def _prepare_features(self, df):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏"""
        feature_cols = [
            'clickbait_score', 'emotional_score', 'certainty_score',
            'formality_score', 'source_indicator_score', 'balance_score',
            'avg_words_per_sentence', 'exclamation_density',
            'caps_ratio', 'has_percentages', 'subjectivity'
        ]
        
        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤
        X = df[feature_cols].fillna(0)
        y = df['is_fake']
        
        return X, y
    
    def predict(self, features):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        if self.model is None:
            return self._rule_based_predict(features)
        
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏
            feature_values = [
                features.get('clickbait_score', 0),
                features.get('emotional_score', 0),
                features.get('certainty_score', 0),
                features.get('formality_score', 0),
                features.get('source_indicator_score', 0),
                features.get('balance_score', 0),
                features.get('avg_words_per_sentence', 0),
                features.get('exclamation_density', 0),
                features.get('caps_ratio', 0),
                features.get('has_percentages', 0),
                features.get('subjectivity', 0)
            ]
            
            # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            X_scaled = self.scaler.transform([feature_values])
            proba = self.model.predict_proba(X_scaled)[0]
            
            # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            adjusted_proba = proba[1]
            if features.get('has_percentages', 0):
                adjusted_proba *= 0.8
            if features.get('source_indicator_score', 0) > 0.3:
                adjusted_proba *= 0.7
            
            return {
                'is_fake': adjusted_proba > 0.5,
                'fake_probability': min(adjusted_proba, 0.99)
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ ML –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            return self._rule_based_predict(features)
    
    def _rule_based_predict(self, features):
        """Rule-based –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞–∫ fallback"""
        fake_score = (
            features.get('clickbait_score', 0) * 0.3 +
            features.get('emotional_score', 0) * 0.2 +
            features.get('certainty_score', 0) * 0.2 +
            features.get('exclamation_density', 0) * 0.1 +
            features.get('caps_ratio', 0) * 0.1 +
            (1 - features.get('formality_score', 0)) * 0.1
        )
        
        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        if features.get('has_percentages', 0):
            fake_score *= 0.7
        if features.get('source_indicator_score', 0) > 0.3:
            fake_score *= 0.6
        
        return {
            'is_fake': fake_score > 0.5,
            'fake_probability': min(fake_score, 0.95)
        }
    
    def highlight_text(self, text):
        """–ü–æ–¥—Å–≤–µ—Ç–∫–∞ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ—Ä–∞–∑ –≤ —Ç–µ–∫—Å—Ç–µ"""
        highlighted = text
        
        # –í—ã–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∏–∫–±–µ–π—Ç-—Å–ª–æ–≤
        for word in self.clickbait_words:
            pattern = re.compile(f'\\b{word}\\b', re.IGNORECASE)
            highlighted = pattern.sub(
                f'<span class="highlight clickbait">{word.upper()}</span>',
                highlighted
            )
        
        # –í—ã–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏—á–Ω–æ—Å—Ç–∏
        for word in self.certainty_words:
            pattern = re.compile(f'\\b{word}\\b', re.IGNORECASE)
            highlighted = pattern.sub(
                f'<span class="highlight certainty">{word.upper()}</span>',
                highlighted
            )
        
        # –í—ã–¥–µ–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        for word in self.source_indicators:
            pattern = re.compile(f'\\b{word}\\b', re.IGNORECASE)
            highlighted = pattern.sub(
                f'<span class="highlight source">{word.upper()}</span>',
                highlighted
            )
        
        # –í—ã–¥–µ–ª–µ–Ω–∏–µ —á–∏—Å–µ–ª –∏ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤
        highlighted = re.sub(
            r'(\d+%?)',
            r'<span class="highlight number">\1</span>',
            highlighted
        )
        
        return highlighted
    
    def calculate_reliability_score(self, features, prediction, text):
        """–†–∞—Å—á–µ—Ç –æ–±—â–µ–≥–æ –±–∞–ª–ª–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏"""
        base_score = 100 - (prediction['fake_probability'] * 100)
        
        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏
        adjustments = 0
        
        # –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏
        if features.get('source_indicator_score', 0) > 0.3:
            adjustments += 10
        if features.get('formality_score', 0) > 0.4:
            adjustments += 8
        if features.get('has_percentages', 0):
            adjustments += 5
        
        # –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏
        if features.get('clickbait_score', 0) > 0.4:
            adjustments -= 10
        if features.get('certainty_score', 0) > 0.5:
            adjustments -= 8
        if features.get('exclamation_density', 0) > 0.5:
            adjustments -= 5
        
        final_score = base_score + adjustments
        return max(0, min(100, round(final_score)))
    
    def generate_explanations(self, features, text):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—ä—è—Å–Ω–µ–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        explanations = []
        
        if features.get('clickbait_score', 0) > 0.3:
            explanations.append("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∫–ª–∏–∫–±–µ–π—Ç-—Å–ª–æ–≤–∞")
        
        if features.get('emotional_score', 0) > 0.5:
            explanations.append("üò† –í—ã—Å–æ–∫–∞—è —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞")
        
        if features.get('certainty_score', 0) > 0.4:
            explanations.append("üéØ –ò–∑–±—ã—Ç–æ—á–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—á–Ω–æ—Å—Ç—å")
        
        if features.get('exclamation_density', 0) > 0.3:
            explanations.append("‚ùó –ú–Ω–æ–≥–æ –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤")
        
        if features.get('source_indicator_score', 0) > 0.2:
            explanations.append("‚úÖ –£–ø–æ–º–∏–Ω–∞—é—Ç—Å—è –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏")
        
        if features.get('formality_score', 0) > 0.3:
            explanations.append("üìù –§–æ—Ä–º–∞–ª—å–Ω—ã–π —Å—Ç–∏–ª—å –∏–∑–ª–æ–∂–µ–Ω–∏—è")
        
        if features.get('has_percentages', 0):
            explanations.append("üìä –ü—Ä–∏–≤–µ–¥–µ–Ω—ã —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ")
        
        # –ï—Å–ª–∏ –Ω–µ—Ç –æ–±—ä—è—Å–Ω–µ–Ω–∏–π, –¥–æ–±–∞–≤–∏—Ç—å –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ
        if not explanations:
            explanations.append("üìä –¢–µ–∫—Å—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —è–≤–Ω—ã—Ö —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –º–∞—Ä–∫–µ—Ä–æ–≤")
        
        return explanations
