import re
import pandas as pd
import numpy as np
from textblob import TextBlob
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
import joblib
import os
import random
from collections import Counter
import math

class StyleAnalyzer:
    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        self.model_accuracy = 0.0
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –≤–µ—Å–∞–º–∏
        self.clickbait_words = {
            '—à–æ–∫': 2.0, '—Å–µ–Ω—Å–∞—Ü–∏—è': 2.0, '—Ç–∞–π–Ω–∞': 1.5, '—Å–∫–∞–Ω–¥–∞–ª': 1.8, '—Ä–∞–∑–æ–±–ª–∞—á–µ–Ω–∏–µ': 1.7,
            '—É–∂–∞—Å': 1.8, '—á—É–¥–æ': 1.3, '–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ': 1.6, '–ø–æ—Ç—Ä—è—Å–∞—é—â–µ': 1.4, '—Å—Ä–æ—á–Ω–æ': 1.5,
            '—ç–∫—Å–∫–ª—é–∑–∏–≤': 1.6, '—Å–µ–∫—Ä–µ—Ç': 1.4, '–ø—Ä–∞–≤–¥–∞': 1.2, '–ª–æ–∂—å': 1.5, '–æ–±–º–∞–Ω': 1.7,
            '–≤—Å–∫—Ä—ã–ª–æ—Å—å': 1.8, '–æ–∫–∞–∑–∞–ª–æ—Å—å': 1.3, '–≤—ã—è—Å–Ω–∏–ª–æ—Å—å': 1.3, '–≤–æ—Ç —á—Ç–æ': 1.2,
            '—à–æ–∫–∏—Ä—É—é—â–∏–π': 2.0, '—Å–µ–Ω—Å–∞—Ü–∏–æ–Ω–Ω—ã–π': 2.0, '–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω—ã–π': 1.7, '–ø–æ—Ç—Ä—è—Å–∞—é—â–∏–π': 1.5,
            '—É–¥–∏–≤–∏—Ç–µ–ª—å–Ω—ã–π': 1.4, '–∂—É—Ç–∫–∏–π': 1.8, '—Å—Ç—Ä–∞—à–Ω—ã–π': 1.7, '–æ–ø–∞—Å–Ω—ã–π': 1.5, '—É–∂–∞—Å–Ω—ã–π': 1.8,
            '–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π': 1.3, '–∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏—á–µ—Å–∫–∏–π': 1.9, '—Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π': 1.4,
            '—Ä–∞–∑–æ–±–ª–∞—á–∏–ª–∏': 1.8, '–æ–±–Ω–∞—Ä—É–∂–∏–ª–∏': 1.3, '–æ—Ç–∫—Ä—ã–ª–∏': 1.2, '–ø—Ä–∏–∑–Ω–∞–ª–∏': 1.3
        }
        
        self.certainty_words = {
            '—Ç–æ—á–Ω–æ': 1.8, '–∞–±—Å–æ–ª—é—Ç–Ω–æ': 1.9, '–Ω–µ—Å–æ–º–Ω–µ–Ω–Ω–æ': 1.8, '–±–µ–∑—É—Å–ª–æ–≤–Ω–æ': 1.7,
            '–∫–æ–Ω–µ—á–Ω–æ': 1.5, '—è–≤–Ω–æ': 1.6, '–æ—á–µ–≤–∏–¥–Ω–æ': 1.7, '–Ω–∞–≤–µ—Ä–Ω—è–∫–∞': 1.5,
            '–≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ': 1.9, '—Å—Ç–æ–ø—Ä–æ—Ü–µ–Ω—Ç–Ω–æ': 1.9, '–¥–æ–∫–∞–∑–∞–Ω–æ': 1.8,
            '—Ñ–∞–∫—Ç': 1.7, '–∏—Å—Ç–∏–Ω–∞': 1.6, '–ø—Ä–∞–≤–¥–∞': 1.4, '—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ': 1.6,
            '–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ': 1.7, '–ø—Ä–æ–≤–µ—Ä–µ–Ω–æ': 1.5, '–¥–æ–∫—É–º–µ–Ω—Ç–∞–ª—å–Ω–æ': 1.4,
            '–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ': 1.3, '–Ω–∞—É—á–Ω–æ': 1.3, '–Ω–µ—Å–æ–º–Ω–µ–Ω–Ω—ã–π': 1.8,
            '–±–µ—Å—Å–ø–æ—Ä–Ω–æ': 1.8, '–Ω–µ–æ–ø—Ä–æ–≤–µ—Ä–∂–∏–º–æ': 1.9
        }
        
        self.formal_words = {
            '—Å–æ–æ–±—â–∏–ª': 1.0, '–∑–∞—è–≤–∏–ª': 1.0, '–æ—Ç–º–µ—Ç–∏–ª': 0.8, '–ø–æ–¥—á–µ—Ä–∫–Ω—É–ª': 0.9,
            '—É–∫–∞–∑–∞–ª': 0.8, '–¥–æ–±–∞–≤–∏–ª': 0.7, '–ø–æ –¥–∞–Ω–Ω—ã–º': 1.2, '—Å–æ–≥–ª–∞—Å–Ω–æ': 1.1,
            '–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏': 1.2, '–Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏': 1.1, '–æ—Ç—á–µ—Ç': 1.0,
            '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ': 1.0, '–∞–Ω–∞–ª–∏–∑': 1.0, '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞': 1.1,
            '—ç–∫—Å–ø–µ—Ä—Ç': 0.9, '—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç': 0.9, '–∞–Ω–∞–ª–∏—Ç–∏–∫': 0.9,
            '–¥–æ–∫–ª–∞–¥': 1.0, '–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è': 0.8, '–ø—Ä–µ—Å—Å-—Ä–µ–ª–∏–∑': 1.0
        }
        
        self.source_indicators = {
            '–ø–æ –¥–∞–Ω–Ω—ã–º': 1.3, '—Å–æ–≥–ª–∞—Å–Ω–æ': 1.2, '–∫–∞–∫ —Å–æ–æ–±—â–∞–µ—Ç': 1.3,
            '–ø–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏': 1.2, '–ø–æ —Å–ª–æ–≤–∞–º': 1.1, '–ø–æ —Å–≤–µ–¥–µ–Ω–∏—è–º': 1.1,
            '–≤ –∏–Ω—Ç–µ—Ä–≤—å—é': 0.9, '–Ω–∞ –ø—Ä–µ—Å—Å-–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏': 1.0,
            '–≤ –∑–∞—è–≤–ª–µ–Ω–∏–∏': 1.0, '–≤ –¥–æ–∫–ª–∞–¥–µ': 1.0, '–≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏': 1.0,
            '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞': 1.0, '–æ—Ç—á–µ—Ç': 1.0, '–∞–Ω–∞–ª–∏–∑': 1.0,
            '—Ü–∏—Ñ—Ä—ã': 0.8, '—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã': 0.8, '–æ–ø—Ä–æ—Å': 0.8
        }
        
        self.balance_indicators = {
            '—Å –æ–¥–Ω–æ–π —Å—Ç–æ—Ä–æ–Ω—ã': 1.2, '—Å –¥—Ä—É–≥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã': 1.2,
            '–æ–¥–Ω–∞–∫–æ': 0.8, '—Ç–µ–º –Ω–µ –º–µ–Ω–µ–µ': 0.9, '–≤–ø—Ä–æ—á–µ–º': 0.7,
            '—Ö–æ—Ç—è': 0.6, '–Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞': 0.7, '–ø–æ –º–Ω–µ–Ω–∏—é': 0.8,
            '–ø–æ –æ—Ü–µ–Ω–∫–∞–º': 0.9, '–≤–æ–∑–º–æ–∂–Ω–æ': 0.7, '–≤–µ—Ä–æ—è—Ç–Ω–æ': 0.7,
            '–ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ': 0.6, '–ø–æ-–≤–∏–¥–∏–º–æ–º—É': 0.6,
            '—Å–æ–≥–ª–∞—Å–Ω–æ –º–Ω–µ–Ω–∏—é': 0.9, '–∏—Å—Ö–æ–¥—è –∏–∑': 0.8
        }
        
        self.emotional_intensifiers = {
            '–æ—á–µ–Ω—å': 1.2, '–∫—Ä–∞–π–Ω–µ': 1.5, '—á—Ä–µ–∑–≤—ã—á–∞–π–Ω–æ': 1.6, '–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ': 1.7,
            '—É–∂–∞—Å–Ω–æ': 1.8, '–∂—É—Ç–∫–æ': 1.8, '—Å—Ç—Ä–∞—à–Ω–æ': 1.7, '–Ω–µ–æ–±—ã—á–∞–π–Ω–æ': 1.5,
            '—Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ': 1.4, '–∞–±—Å–æ–ª—é—Ç–Ω–æ': 1.6, '–ø–æ–ª–Ω–æ—Å—Ç—å—é': 1.3,
            '—Å–∏–ª—å–Ω–æ': 1.2, '–≥–ª—É–±–æ–∫–æ': 1.1, '–Ω–µ–æ–±—ã—á–Ω–æ': 1.2
        }
        
        self.news_sources = {
            '—Ü–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫': 1.4, '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ': 1.3, '–º–∏–Ω–∑–¥—Ä–∞–≤': 1.2,
            '—Ä–æ—Å–ø–æ—Ç—Ä–µ–±–Ω–∞–¥–∑–æ—Ä': 1.2, '—Ä–æ—Å—Å—Ç–∞—Ç': 1.3, '–æ–æ–Ω': 1.1,
            '–≤—Å–µ–º–∏—Ä–Ω—ã–π –±–∞–Ω–∫': 1.1, '–º–≤—Ñ': 1.1, '—ç–∫—Å–ø–µ—Ä—Ç—ã': 0.9,
            '–∞–Ω–∞–ª–∏—Ç–∏–∫–∏': 0.9, '—É—á–µ–Ω—ã–µ': 0.9, '–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏': 0.9,
            '–∂—É—Ä–Ω–∞–ª–∏—Å—Ç—ã': 0.7, '–∫–æ—Ä—Ä–µ—Å–ø–æ–Ω–¥–µ–Ω—Ç—ã': 0.7, '—Ä–µ–¥–∞–∫—Ü–∏—è': 0.7,
            '–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ': 1.1, '–≤–µ–¥–æ–º—Å—Ç–≤–æ': 1.0, '–∞–≥–µ–Ω—Ç—Å—Ç–≤–æ': 1.0
        }
        
        self.credibility_indicators = {
            '–ø—Ä–æ—Ü–µ–Ω—Ç': 0.8, '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞': 0.9, '–¥–∞–Ω–Ω—ã–µ': 0.8,
            '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ': 0.9, '–∞–Ω–∞–ª–∏–∑': 0.9, '–æ—Ç—á–µ—Ç': 0.9,
            '—Ü–∏—Ñ—Ä—ã': 0.8, '–ø–æ–∫–∞–∑–∞—Ç–µ–ª—å': 0.8, '—Ç–µ–Ω–¥–µ–Ω—Ü–∏—è': 0.7,
            '–¥–∏–Ω–∞–º–∏–∫–∞': 0.7, '—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã': 0.8, '–º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è': 0.6
        }
        
    def extract_features(self, text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        features = {}
        text_lower = text.lower()
        
        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        features['length'] = len(text)
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        words = re.findall(r'\b\w+\b', text_lower)
        features['word_count'] = len(words)
        
        if sentences:
            features['sentence_count'] = len(sentences)
            features['avg_sentence_length'] = sum(len(s) for s in sentences) / len(sentences)
            features['avg_words_per_sentence'] = len(words) / len(sentences) if len(sentences) > 0 else 0
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            if len(sentences) > 1:
                sentence_lengths = [len(s) for s in sentences]
                features['sentence_length_std'] = np.std(sentence_lengths)
            else:
                features['sentence_length_std'] = 0
        else:
            features['sentence_count'] = 0
            features['avg_sentence_length'] = 0
            features['avg_words_per_sentence'] = 0
            features['sentence_length_std'] = 0
        
        # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å (—Å–µ–Ω—Ç–∏–º–µ–Ω—Ç-–∞–Ω–∞–ª–∏–∑ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
        try:
            blob = TextBlob(text)
            features['emotional_score'] = min(abs(blob.sentiment.polarity) * 1.5, 1.0)
            features['subjectivity'] = blob.sentiment.subjectivity
        except:
            features['emotional_score'] = 0.0
            features['subjectivity'] = 0.0
        
        # –£—Å–∏–ª–∏—Ç–µ–ª–∏ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        intensifier_score = sum(self.emotional_intensifiers.get(word, 0) for word in words)
        features['intensifier_score'] = min(intensifier_score / 3, 1.0)
        
        # –ö–ª–∏–∫–±–µ–π—Ç –∏–Ω–¥–µ–∫—Å —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –≤–µ—Å–æ–≤
        clickbait_score = 0
        clickbait_words_found = []
        for word, weight in self.clickbait_words.items():
            if word in text_lower:
                clickbait_score += weight
                clickbait_words_found.append(word)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å —É—á–µ—Ç–æ–º –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞
        features['clickbait_score'] = min(clickbait_score / max(len(words) / 50, 1), 1.5)
        features['clickbait_words'] = clickbait_words_found
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏—á–Ω–æ—Å—Ç—å —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏ –≤–µ—Å–∞–º–∏
        certainty_score = 0
        certainty_words_found = []
        for word, weight in self.certainty_words.items():
            if word in text_lower:
                certainty_score += weight
                certainty_words_found.append(word)
        
        # –£—á–∏—Ç—ã–≤–∞–µ–º –¥–ª–∏–Ω—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        sentence_penalty = 1.0
        if features['avg_words_per_sentence'] < 12:
            sentence_penalty = 1.3
        elif features['avg_words_per_sentence'] > 30:
            sentence_penalty = 0.7
            
        features['certainty_score'] = min(certainty_score * sentence_penalty / 4, 1.5)
        features['certainty_words'] = certainty_words_found
        
        # –§–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—å —Å—Ç–∏–ª—è
        formality_score = sum(self.formal_words.get(word, 0) for word in words)
        features['formality_score'] = min(formality_score / 5, 1.0)
        
        # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        source_score = sum(self.source_indicators.get(word, 0) for word in words)
        features['source_indicator_score'] = min(source_score / 3, 1.0)
        
        # –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∏–∑–ª–æ–∂–µ–Ω–∏—è
        balance_score = sum(self.balance_indicators.get(word, 0) for word in words)
        features['balance_score'] = min(balance_score / 2, 1.0)
        
        # –ù–∞–ª–∏—á–∏–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        news_source_score = sum(self.news_sources.get(word, 0) for word in words)
        features['news_source_score'] = min(news_source_score / 2, 1.0)
        
        # –ü–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ (—Ü–∏—Ñ—Ä—ã, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞)
        credibility_score = sum(self.credibility_indicators.get(word, 0) for word in words)
        features['credibility_indicator_score'] = min(credibility_score / 3, 1.0)
        
        # –ü—É–Ω–∫—Ç—É–∞—Ü–∏—è –∏ —Ä–µ–≥–∏—Å—Ç—Ä
        features['exclamation_count'] = text.count('!')
        features['question_count'] = text.count('?')
        features['exclamation_density'] = features['exclamation_count'] / max(len(sentences), 1)
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–≥–∏—Å—Ç—Ä–∞
        caps_count = sum(1 for c in text if c.isupper())
        features['caps_count'] = caps_count
        features['caps_ratio'] = caps_count / len(text) if len(text) > 0 else 0
        
        # –ù–∞–ª–∏—á–∏–µ CAPS LOCK —Ñ—Ä–∞–∑ (—Å–ª–æ–≤–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≥–ª–∞–≤–Ω—ã–º–∏)
        caps_words = re.findall(r'\b[A-Z–ê-–Ø]{2,}\b', text)
        features['caps_lock_words'] = len(caps_words)
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        numbers = re.findall(r'\b\d+\b', text)
        features['number_count'] = len(numbers)
        features['has_percentages'] = 1 if ('%' in text or '–ø—Ä–æ—Ü–µ–Ω—Ç' in text_lower or '–ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤' in text_lower) else 0
        features['has_dates'] = 1 if bool(re.search(r'\d{1,2}[./]\d{1,2}[./]\d{2,4}', text)) else 0
        features['has_currency'] = 1 if bool(re.search(r'\d+\s*(—Ä—É–±|—Ä|‚ÇΩ|usd|\$|‚Ç¨|–µ–≤—Ä–æ)', text_lower)) else 0
        
        # –õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
        if words:
            unique_words = set(words)
            features['lexical_diversity'] = len(unique_words) / len(words)
            features['unique_word_count'] = len(unique_words)
            # –ß–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å —Å–ª–æ–≤
            word_freq = Counter(words)
            most_common = word_freq.most_common(3)
            features['most_common_words'] = [word for word, count in most_common]
            features['word_repetition_score'] = min(sum(count for _, count in most_common) / len(words) * 3, 1.0)
        else:
            features['lexical_diversity'] = 0
            features['unique_word_count'] = 0
            features['most_common_words'] = []
            features['word_repetition_score'] = 0
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏
        features['paragraph_count'] = text.count('\n\n') + 1
        features['has_lists'] = 1 if bool(re.search(r'\d+\.\s|\-\s|\*\s', text)) else 0
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features['is_question'] = 1 if text.strip().endswith('?') else 0
        features['title_case_ratio'] = self._calculate_title_case_ratio(text)
        
        # –°–ª–æ–∂–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞)
        features['complexity_score'] = self._calculate_complexity_score(text, words, sentences)
        
        return features
    
    def _calculate_title_case_ratio(self, text):
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Å–ª–æ–≤ —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã –∫ –æ–±—â–µ–º—É —á–∏—Å–ª—É —Å–ª–æ–≤"""
        words = re.findall(r'\b[A-Z–ê-–Ø][a-z–∞-—è]*\b', text)
        all_words = re.findall(r'\b[a-zA-Z–∞-—è–ê-–Ø]+\b', text)
        if all_words:
            return len(words) / len(all_words)
        return 0
    
    def _calculate_complexity_score(self, text, words, sentences):
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞"""
        if not words or not sentences:
            return 0.0
            
        # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞
        avg_word_length = sum(len(word) for word in words) / len(words)
        
        # –î–æ–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Å–ª–æ–≤ (–±–æ–ª–µ–µ 6 —Å–∏–º–≤–æ–ª–æ–≤)
        long_words = sum(1 for word in words if len(word) > 6)
        long_word_ratio = long_words / len(words)
        
        # –ò–Ω–¥–µ–∫—Å —É–¥–æ–±–æ—á–∏—Ç–∞–µ–º–æ—Å—Ç–∏ Flesch-Kincaid (–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)
        avg_sentence_length = len(words) / len(sentences)
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        complexity = (
            min(avg_word_length / 10, 1.0) * 0.4 +
            min(long_word_ratio * 2, 1.0) * 0.3 +
            min(avg_sentence_length / 30, 1.0) * 0.3
        )
        
        return min(complexity, 1.0)
    
    def generate_synthetic_data(self, n_samples=1500):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        data = []
        
        # –®–∞–±–ª–æ–Ω—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ç–µ–∫—Å—Ç–æ–≤
        fake_templates = [
            "–®–û–ö! {subject} —Å–∫—Ä—ã–≤–∞—é—Ç –ü–†–ê–í–î–£ –æ {topic}! –í—Å–µ –≤ –£–ñ–ê–°–ï! –ê–ë–°–û–õ–Æ–¢–ù–û —Ç–æ—á–Ω–æ!",
            "–°–ï–ù–°–ê–¶–ò–Ø! {subject} –†–ê–ó–û–ë–õ–ê–ß–ò–õ–ò {topic}! –≠—Ç–æ –ù–ï–í–ï–†–û–Ø–¢–ù–û! –°–†–û–ß–ù–û!",
            "–¢–ê–ô–ù–ê {topic} –†–ê–°–ö–†–´–¢–ê! {subject} –≤ –®–û–ö–ï! –î–û–ö–ê–ó–ê–ù–û –Ω–∞—É—á–Ω–æ!",
            "{subject} –û–ë–ú–ê–ù–´–í–ê–Æ–¢ –Ω–∞—Å! –ñ–£–¢–ö–ê–Ø –ü–†–ê–í–î–ê –æ {topic}!",
            "–í–°–ö–†–´–õ–ê–°–¨ –£–ñ–ê–°–ù–ê–Ø –¢–ê–ô–ù–ê! {topic} - —ç—Ç–æ –û–ü–ê–°–ù–û–°–¢–¨!",
            "–ù–ï–í–ï–†–û–Ø–¢–ù–û! {subject} –°–ö–†–´–í–ê–Æ–¢ {topic}! –≠—Ç–æ –§–ê–ö–¢!",
            "–®–û–ö–ò–†–£–Æ–©–ï–ï –û–¢–ö–†–´–¢–ò–ï! {topic} –£–ë–ò–í–ê–ï–¢! –°–ï–ù–°–ê–¶–ò–Ø!",
            "{subject} –≤ –ü–ê–ù–ò–ö–ï! {topic} –£–ì–†–û–ñ–ê–ï–¢ –≤—Å–µ–º! –°–ö–ê–ù–î–ê–õ!",
            "–ú–ò–† –ü–ï–†–ï–í–ï–†–ù–£–õ–°–Ø! {topic} –æ–∫–∞–∑–∞–ª—Å—è –û–ë–ú–ê–ù–û–ú!",
            "–ñ–£–¢–¨! {subject} –ú–û–õ–ß–ê–¢ –æ {topic}! –≠—Ç–æ –ö–ê–¢–ê–°–¢–†–û–§–ê!"
        ]
        
        # –†–µ–∞–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ —Å —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
        real_emotional_templates = [
            "–°–ï–ù–°–ê–¶–ò–Ø: {source} –ø–æ–≤—ã—Å–∏–ª —Å—Ç–∞–≤–∫—É –¥–æ {number}%",
            "–í–ê–ñ–ù–û: –ü–æ –¥–∞–Ω–Ω—ã–º {source}, {topic} –¥–æ—Å—Ç–∏–≥ {number}%",
            "{source} —Å–æ–æ–±—â–∞–µ—Ç –æ —Ä–æ—Å—Ç–µ {topic} –Ω–∞ {number}%",
            "–≠–ö–°–ö–õ–Æ–ó–ò–í: {source} –æ–ø—É–±–ª–∏–∫–æ–≤–∞–ª –¥–∞–Ω–Ω—ã–µ –ø–æ {topic}",
            "–°–æ–≥–ª–∞—Å–Ω–æ –æ—Ç—á–µ—Ç—É {source}, {topic} —Å–æ—Å—Ç–∞–≤–∏–ª {number}%",
            "{source} –∑–∞—è–≤–∏–ª –æ–± –∏–∑–º–µ–Ω–µ–Ω–∏–∏ {topic} –¥–æ {number}%",
            "–ü–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ {source}, {topic} –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ä–æ—Å—Ç",
            "{source} –æ–±–Ω–∞—Ä–æ–¥–æ–≤–∞–ª —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ {topic}",
            "–í —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –¥–∞–Ω–Ω—ã–º–∏ {source}, {topic} —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞–ª—Å—è",
            "–≠–∫—Å–ø–µ—Ä—Ç—ã {source} –ø—Ä–æ–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–ª–∏ —Å–∏—Ç—É–∞—Ü–∏—é —Å {topic}"
        ]
        
        # –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
        neutral_templates = [
            "–ü–æ –¥–∞–Ω–Ω—ã–º {source}, {topic} —Å–æ—Å—Ç–∞–≤–∏–ª {number}% –∑–∞ –æ—Ç—á–µ—Ç–Ω—ã–π –ø–µ—Ä–∏–æ–¥.",
            "–≠–∫—Å–ø–µ—Ä—Ç—ã –æ—Ç–º–µ—á–∞—é—Ç, —á—Ç–æ {topic} –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É—Å—Ç–æ–π—á–∏–≤—ã–π —Ä–æ—Å—Ç.",
            "–í —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –æ—Ç—á–µ—Ç–æ–º {source}, {topic} –æ—Å—Ç–∞–ª—Å—è –Ω–∞ –ø—Ä–µ–∂–Ω–µ–º —É—Ä–æ–≤–Ω–µ.",
            "–ê–Ω–∞–ª–∏—Ç–∏–∫–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É—é—Ç —É–º–µ—Ä–µ–Ω–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ {topic} –≤ –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è.",
            "–°–æ–≥–ª–∞—Å–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—é, {topic} –∏–º–µ–µ—Ç —Ç–µ–Ω–¥–µ–Ω—Ü–∏—é –∫ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–º—É —É–ª—É—á—à–µ–Ω–∏—é.",
            "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ {topic} —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Å—Ä–µ–¥–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏—è–º.",
            "–í —Ö–æ–¥–µ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ –æ–±—Å—É–∂–¥–∞–ª–∏—Å—å –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã —Ä–∞–∑–≤–∏—Ç–∏—è {topic}.",
            "–î–æ–∫–ª–∞–¥ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∞–Ω–Ω—ã–µ –æ —Ç–µ–∫—É—â–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏ {topic} –≤ —Ä–µ–≥–∏–æ–Ω–µ.",
            "–≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤—É—é—Ç –æ —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ {topic}.",
            "–ü–æ –º–Ω–µ–Ω–∏—é —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤, {topic} —Ç—Ä–µ–±—É–µ—Ç –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏–∑—É—á–µ–Ω–∏—è."
        ]
        
        # –î–µ–∑–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏ –ø—Ä–∞–≤–¥—ã
        mixed_templates = [
            "–≠–∫—Å–ø–µ—Ä—Ç—ã –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞—é—Ç: {topic} –º–æ–∂–µ—Ç –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—å {number}%",
            "–ù–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –æ {topic}: —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—ã –æ–±—Å—É–∂–¥–∞—é—Ç –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è",
            "–ê–Ω–∞–ª–∏—Ç–∏–∫–∏ —Å–ø–æ—Ä—è—Ç –æ –≤–ª–∏—è–Ω–∏–∏ {topic} –Ω–∞ —ç–∫–æ–Ω–æ–º–∏–∫—É",
            "–í —Å–≤—è–∑–∏ —Å {topic} –≤–æ–∑–º–æ–∂–Ω—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–µ",
            "–î–∏—Å–∫—É—Å—Å–∏—è –æ {topic} –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è —Å—Ä–µ–¥–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤"
        ]
        
        subjects = ["—É—á–µ–Ω—ã–µ", "–≤–ª–∞—Å—Ç–∏", "–≤—Ä–∞—á–∏", "–ø–æ–ª–∏—Ç–∏–∫–∏", "–∂—É—Ä–Ω–∞–ª–∏—Å—Ç—ã", "–±–∞–Ω–∫–∏", "–∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏"]
        topics = ["–∏–Ω—Ñ–ª—è—Ü–∏–∏", "—ç–∫–æ–Ω–æ–º–∏–∫–µ", "–∫–ª–∏–º–∞—Ç–µ", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö", "–∑–¥–æ—Ä–æ–≤—å–µ", "—Ñ–∏–Ω–∞–Ω—Å–∞—Ö", "–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏", "–≤–∞–∫—Ü–∏–Ω–∞—Ü–∏–∏"]
        sources = ["–¶–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫–∞", "–ú–∏–Ω–∑–¥—Ä–∞–≤–∞", "–†–æ—Å—Å—Ç–∞—Ç–∞", "–û–û–ù", "–í–û–ó", "—ç–∫—Å–ø–µ—Ä—Ç–æ–≤", "–∞–Ω–∞–ª–∏—Ç–∏–∫–æ–≤", "–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π"]
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        n_fake = n_samples // 3
        n_real_emotional = n_samples // 3
        n_neutral = n_samples // 4
        n_mixed = n_samples - n_fake - n_real_emotional - n_neutral
        
        # –§–µ–π–∫–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
        for _ in range(n_fake):
            template = random.choice(fake_templates)
            text = template.format(
                subject=random.choice(subjects),
                topic=random.choice(topics),
                source=random.choice(sources),
                number=random.randint(1, 99)
            )
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
            if random.random() > 0.6:
                text = text.upper()
            if random.random() > 0.4:
                text = text + " " + "!" * random.randint(1, 3)
            if random.random() > 0.8:
                text = "–í–ù–ò–ú–ê–ù–ò–ï! " + text
            
            features = self.extract_features(text)
            features['text'] = text
            features['is_fake'] = 1
            data.append(features)
        
        # –†–µ–∞–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ —Å —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
        for _ in range(n_real_emotional):
            template = random.choice(real_emotional_templates)
            text = template.format(
                source=random.choice(sources),
                topic=random.choice(topics),
                number=random.randint(1, 99)
            )
            
            features = self.extract_features(text)
            features['text'] = text
            features['is_fake'] = 0
            data.append(features)
        
        # –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
        for _ in range(n_neutral):
            template = random.choice(neutral_templates)
            text = template.format(
                source=random.choice(sources),
                topic=random.choice(topics),
                number=random.randint(1, 99)
            )
            
            # –ò–Ω–æ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–µ–º –Ω–µ–º–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç–∏
            if random.random() > 0.7:
                text = "–û—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ: " + text
            
            features = self.extract_features(text)
            features['text'] = text
            features['is_fake'] = 0
            data.append(features)
        
        # –°–º–µ—à–∞–Ω–Ω—ã–µ (–¥–µ–∑–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è)
        for _ in range(n_mixed):
            template = random.choice(mixed_templates)
            text = template.format(
                topic=random.choice(topics),
                number=random.randint(1, 99)
            )
            
            # –°–ª—É—á–∞–π–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞–∫ —Ñ–µ–π–∫ –∏–ª–∏ –ø—Ä–∞–≤–¥—É
            is_fake = random.random() > 0.6
            
            features = self.extract_features(text)
            features['text'] = text
            features['is_fake'] = 1 if is_fake else 0
            data.append(features)
        
        df = pd.DataFrame(data)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏
        for i in range(len(df)):
            if random.random() < 0.05:  # 5% —à—É–º–∞
                df.loc[i, 'is_fake'] = 1 - df.loc[i, 'is_fake']
        
        return df
    
    def load_or_train_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏ –∏–ª–∏ –æ–±—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ–π"""
        model_path = 'models/fake_news_model.pkl'
        scaler_path = 'models/scaler.pkl'
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –º–æ–¥–µ–ª–µ–π
        os.makedirs('models', exist_ok=True)
        
        try:
            # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –º–æ–¥–µ–ª—å
            if os.path.exists(model_path) and os.path.exists(scaler_path):
                self.model = joblib.load(model_path)
                self.scaler = joblib.load(scaler_path)
                print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ —Ñ–∞–π–ª–∞")
                
                # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                df = self.generate_synthetic_data(300)
                X, y = self._prepare_features(df)
                X_scaled = self.scaler.transform(X)
                accuracy = self.model.score(X_scaled, y)
                self.model_accuracy = accuracy
                print(f"üìä –¢–æ—á–Ω–æ—Å—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏: {accuracy:.2%}")
                return
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å: {e}")
        
        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å, –æ–±—É—á–∞–µ–º –Ω–æ–≤—É—é
        print("üîß –û–±—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏...")
        self.train_model()
    
    def train_model(self):
        """–û–±—É—á–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π ML –º–æ–¥–µ–ª–∏"""
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        df = self.generate_synthetic_data(1500)
        print(f"üìà –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(df)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        X, y = self._prepare_features(df)
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        print(f"üìö –†–∞–∑–º–µ—Ä —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏: {len(X_train)}")
        print(f"üìö –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {len(X_test)}")
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏
        self.model = GradientBoostingClassifier(
            n_estimators=300,
            learning_rate=0.05,
            max_depth=7,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            subsample=0.8,
            max_features='sqrt'
        )
        
        # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
        print("üî¨ –ü—Ä–æ–≤–æ–¥–∏–º –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é...")
        cv_scores = cross_val_score(self.model, X_train_scaled, y_train, cv=5)
        print(f"üìä –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è: {cv_scores.mean():.2%} (+/- {cv_scores.std() * 2:.2%})")
        
        # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        print("üéì –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å...")
        self.model.fit(X_train_scaled, y_train)
        
        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        train_accuracy = self.model.score(X_train_scaled, y_train)
        test_accuracy = self.model.score(X_test_scaled, y_test)
        self.model_accuracy = test_accuracy
        
        print(f"üìà –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {train_accuracy:.2%}")
        print(f"üìà –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {test_accuracy:.2%}")
        
        # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_importance = pd.DataFrame({
            'feature': X.columns,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print("\nüèÜ –¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
        for idx, row in feature_importance.head(10).iterrows():
            print(f"  {row['feature']}: {row['importance']:.3f}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        joblib.dump(self.model, 'models/fake_news_model.pkl')
        joblib.dump(self.scaler, 'models/scaler.pkl')
        
        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ models/fake_news_model.pkl")
        
        return self.model
    
    def _prepare_features(self, df):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏"""
        feature_cols = [
            'clickbait_score', 'emotional_score', 'certainty_score',
            'formality_score', 'source_indicator_score', 'balance_score',
            'news_source_score', 'credibility_indicator_score',
            'avg_words_per_sentence', 'exclamation_density',
            'caps_ratio', 'caps_lock_words', 'has_percentages',
            'lexical_diversity', 'intensifier_score', 'subjectivity',
            'word_repetition_score', 'complexity_score', 'sentence_length_std'
        ]
        
        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤
        X = df[feature_cols].fillna(0)
        y = df['is_fake']
        
        return X, y
    
    def predict(self, features):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        if self.model is None:
            # Fallback –Ω–∞ rule-based –ø–æ–¥—Ö–æ–¥
            return self._rule_based_predict(features)
        
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏
            feature_values = self._prepare_prediction_features(features)
            
            # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            X_scaled = self.scaler.transform([feature_values])
            proba = self.model.predict_proba(X_scaled)[0]
            
            # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            adjusted_proba = self._adjust_probability(proba[1], features)
            
            return {
                'is_fake': adjusted_proba > 0.5,
                'fake_probability': adjusted_proba,
                'raw_probability': proba[1]
            }
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ ML –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            return self._rule_based_predict(features)
    
    def _prepare_prediction_features(self, features):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
        return [
            features.get('clickbait_score', 0),
            features.get('emotional_score', 0),
            features.get('certainty_score', 0),
            features.get('formality_score', 0),
            features.get('source_indicator_score', 0),
            features.get('balance_score', 0),
            features.get('news_source_score', 0),
            features.get('credibility_indicator_score', 0),
            features.get('avg_words_per_sentence', 0),
            features.get('exclamation_density', 0),
            features.get('caps_ratio', 0),
            features.get('caps_lock_words', 0),
            features.get('has_percentages', 0),
            features.get('lexical_diversity', 0),
            features.get('intensifier_score', 0),
            features.get('subjectivity', 0),
            features.get('word_repetition_score', 0),
            features.get('complexity_score', 0),
            features.get('sentence_length_std', 0)
        ]
    
    def _adjust_probability(self, probability, features):
        """–ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        adjusted = probability
        
        # –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ (—Å–Ω–∏–∂–∞—é—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ñ–µ–π–∫–∞)
        if features.get('source_indicator_score', 0) > 0.3:
            adjusted *= 0.7
        if features.get('credibility_indicator_score', 0) > 0.3:
            adjusted *= 0.8
        if features.get('formality_score', 0) > 0.4:
            adjusted *= 0.6
        if features.get('has_percentages', 0) > 0:
            adjusted *= 0.8
        if features.get('news_source_score', 0) > 0.3:
            adjusted *= 0.7
        if features.get('balance_score', 0) > 0.3:
            adjusted *= 0.9
        
        # –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ (–ø–æ–≤—ã—à–∞—é—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ñ–µ–π–∫–∞)
        if features.get('clickbait_score', 0) > 0.5:
            adjusted = min(adjusted * 1.5, 0.95)
        if features.get('emotional_score', 0) > 0.6:
            adjusted = min(adjusted * 1.3, 0.95)
        if features.get('certainty_score', 0) > 0.5:
            adjusted = min(adjusted * 1.4, 0.95)
        if features.get('exclamation_density', 0) > 0.5:
            adjusted = min(adjusted * 1.2, 0.95)
        if features.get('caps_ratio', 0) > 0.3:
            adjusted = min(adjusted * 1.3, 0.95)
        if features.get('intensifier_score', 0) > 0.5:
            adjusted = min(adjusted * 1.2, 0.95)
        
        # –£—á–∏—Ç—ã–≤–∞–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞
        if features.get('complexity_score', 0) < 0.2 and len(features.get('most_common_words', [])) > 0:
            # –û—á–µ–Ω—å –ø—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç —Å –ø–æ–≤—Ç–æ—Ä–∞–º–∏
            adjusted = min(adjusted * 1.1, 0.95)
        
        return min(max(adjusted, 0.01), 0.99)
    
    def _rule_based_predict(self, features):
        """Rule-based –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞–∫ fallback"""
        fake_score = (
            features.get('clickbait_score', 0) * 0.25 +
            features.get('emotional_score', 0) * 0.20 +
            features.get('certainty_score', 0) * 0.15 +
            features.get('exclamation_density', 0) * 0.15 +
            features.get('caps_ratio', 0) * 0.10 +
            (1 - features.get('formality_score', 0)) * 0.10 +
            (1 - features.get('source_indicator_score', 0)) * 0.05
        )
        
        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        if features.get('has_percentages', 0) > 0:
            fake_score *= 0.8
        if features.get('news_source_score', 0) > 0.3:
            fake_score *= 0.7
        if features.get('credibility_indicator_score', 0) > 0.3:
            fake_score *= 0.8
        
        return {
            'is_fake': fake_score > 0.55,
            'fake_probability': min(fake_score, 0.95)
        }
    
    def highlight_text(self, text):
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –≤—ã–¥–µ–ª–µ–Ω–∏–µ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ—Ä–∞–∑ –≤ —Ç–µ–∫—Å—Ç–µ"""
        if not text:
            return ""
            
        highlighted = text
        
        # –°–Ω–∞—á–∞–ª–∞ –≤—ã–¥–µ–ª—è–µ–º —á–∏—Å–ª–∞ –∏ –ø—Ä–æ—Ü–µ–Ω—Ç—ã (–∑–µ–ª–µ–Ω—ã–º)
        highlighted = re.sub(
            r'(\d+%?)',
            r'<span class="highlight number" title="–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ">\1</span>',
            highlighted
        )
        
        # –í—ã–¥–µ–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (—Å–∏–Ω–∏–º)
        for source in list(self.news_sources.keys()) + list(self.source_indicators.keys()):
            pattern = re.compile(f'(?<!\\w)({source})(?!\\w)', re.IGNORECASE)
            highlighted = pattern.sub(
                f'<span class="highlight source" title="–£–ø–æ–º–∏–Ω–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞">\\1</span>',
                highlighted
            )
        
        # –í—ã–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∏–∫–±–µ–π—Ç-—Å–ª–æ–≤ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        for word in self.clickbait_words:
            pattern = re.compile(f'(?<!\\w)({word})(?!\\w)', re.IGNORECASE)
            matches = list(pattern.finditer(highlighted))
            
            for match in reversed(matches):
                start, end = match.span()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ —Å–ª–æ–≤–∞
                context_start = max(0, start - 30)
                context_end = min(len(highlighted), end + 30)
                context = highlighted[context_start:context_end].lower()
                
                # –ï—Å–ª–∏ —Ä—è–¥–æ–º –µ—Å—Ç—å —á–∏—Å–ª–∞ –∏–ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ - –º–µ–Ω–µ–µ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ
                has_numbers_nearby = bool(re.search(r'\d+%?', highlighted[max(0, start-20):end+20]))
                has_sources_nearby = any(src in context for src in self.source_indicators)
                
                if has_numbers_nearby and has_sources_nearby:
                    replacement = f'<span class="highlight clickbait-context" title="–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–∞–Ω–Ω—ã—Ö">\\1</span>'
                else:
                    replacement = f'<span class="highlight clickbait" title="–ö–ª–∏–∫–±–µ–π—Ç-—Å–ª–æ–≤–æ">\\1</span>'
                
                highlighted = highlighted[:start] + replacement + highlighted[end:]
        
        # –í—ã–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏—á–Ω–æ—Å—Ç–∏
        for word in self.certainty_words:
            pattern = re.compile(f'(?<!\\w)({word})(?!\\w)', re.IGNORECASE)
            highlighted = pattern.sub(
                f'<span class="highlight certainty" title="–ö–∞—Ç–µ–≥–æ—Ä–∏—á–Ω–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ">\\1</span>',
                highlighted
            )
        
        # –í—ã–¥–µ–ª–µ–Ω–∏–µ –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤
        sentences = re.split(r'([.!?]+)', highlighted)
        for i in range(len(sentences)):
            if '!' in sentences[i]:
                excl_count = sentences[i].count('!')
                if excl_count > 2:
                    sentences[i] = f'<span class="highlight exclamation-high" title="–ú–Ω–æ–≥–æ –≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏–π ({excl_count})">{sentences[i]}</span>'
                elif excl_count > 1:
                    sentences[i] = f'<span class="highlight exclamation-medium" title="–ù–µ—Å–∫–æ–ª—å–∫–æ –≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏–π">{sentences[i]}</span>'
                else:
                    sentences[i] = f'<span class="highlight exclamation" title="–í–æ—Å–∫–ª–∏—Ü–∞–Ω–∏–µ">{sentences[i]}</span>'
            elif '?' in sentences[i]:
                sentences[i] = f'<span class="highlight question" title="–í–æ–ø—Ä–æ—Å">{sentences[i]}</span>'
        
        highlighted = ''.join(sentences)
        
        # –í—ã–¥–µ–ª–µ–Ω–∏–µ –í–°–ï–• –ó–ê–ì–õ–ê–í–ù–´–• –°–õ–û–í
        words = highlighted.split()
        for i, word in enumerate(words):
            if len(word) > 2 and word.isupper() and word.isalpha():
                if len(word) > 5:
                    words[i] = f'<span class="highlight caps-high" title="–î–ª–∏–Ω–Ω–æ–µ —Å–ª–æ–≤–æ –∑–∞–≥–ª–∞–≤–Ω—ã–º–∏">{word}</span>'
                else:
                    words[i] = f'<span class="highlight caps" title="–°–ª–æ–≤–æ –∑–∞–≥–ª–∞–≤–Ω—ã–º–∏">{word}</span>'
        
        highlighted = ' '.join(words)
        
        return highlighted
    
    def calculate_reliability_score(self, features, prediction, text):
        """–†–∞—Å—á–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –±–∞–ª–ª–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏"""
        # –ë–∞–∑–æ–≤—ã–π –±–∞–ª–ª –æ—Ç ML –º–æ–¥–µ–ª–∏
        base_score = 100 - (prediction['fake_probability'] * 100)
        
        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        adjustments = 0
        
        # –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ (–ø–æ–≤—ã—à–∞—é—Ç –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å)
        if features.get('source_indicator_score', 0) > 0.3:
            adjustments += 12
        if features.get('credibility_indicator_score', 0) > 0.3:
            adjustments += 10
        if features.get('formality_score', 0) > 0.4:
            adjustments += 15
        if features.get('has_percentages', 0) > 0:
            adjustments += 8
        if features.get('news_source_score', 0) > 0.3:
            adjustments += 15
        if features.get('balance_score', 0) > 0.3:
            adjustments += 10
        if features.get('has_dates', 0) > 0:
            adjustments += 5
        if features.get('has_currency', 0) > 0:
            adjustments += 5
        
        # –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ (—Å–Ω–∏–∂–∞—é—Ç –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å)
        if features.get('clickbait_score', 0) > 0.6:
            adjustments -= 20
        elif features.get('clickbait_score', 0) > 0.3:
            adjustments -= 10
            
        if features.get('certainty_score', 0) > 0.6:
            adjustments -= 15
        elif features.get('certainty_score', 0) > 0.3:
            adjustments -= 8
            
        if features.get('exclamation_density', 0) > 0.5:
            adjustments -= 12
        elif features.get('exclamation_density', 0) > 0.2:
            adjustments -= 6
            
        if features.get('caps_ratio', 0) > 0.3:
            adjustments -= 15
        elif features.get('caps_ratio', 0) > 0.15:
            adjustments -= 8
            
        if features.get('intensifier_score', 0) > 0.5:
            adjustments -= 10
            
        if features.get('caps_lock_words', 0) > 2:
            adjustments -= 8
            
        if features.get('word_repetition_score', 0) > 0.4:
            adjustments -= 5
        
        # –£—á–∏—Ç—ã–≤–∞–µ–º –¥–ª–∏–Ω—É –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞
        text_length = len(text)
        if text_length > 800:
            adjustments += 10  # –î–ª–∏–Ω–Ω—ã–µ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ–∫—Å—Ç—ã
        elif text_length > 300:
            adjustments += 5
        elif text_length < 100:
            adjustments -= 10  # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã
            
        if features.get('complexity_score', 0) > 0.5:
            adjustments += 5  # –°–ª–æ–∂–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –æ–±—ã—á–Ω–æ –±–æ–ª–µ–µ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω—ã
        
        final_score = base_score + adjustments
        return max(0, min(100, round(final_score)))
    
    def generate_explanations(self, features, text):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—ä—è—Å–Ω–µ–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞"""
        explanations = []
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        total_flags = 0
        warning_flags = 0
        positive_flags = 0
        
        # –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        positive_points = []
        if features.get('source_indicator_score', 0) > 0.2:
            positive_points.append("üìã –£–ø–æ–º–∏–Ω–∞—é—Ç—Å—è –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏")
            positive_flags += 1
        if features.get('formality_score', 0) > 0.3:
            positive_points.append("üìù –§–æ—Ä–º–∞–ª—å–Ω—ã–π —Å—Ç–∏–ª—å –∏–∑–ª–æ–∂–µ–Ω–∏—è")
            positive_flags += 1
        if features.get('balance_score', 0) > 0.2:
            positive_points.append("‚öñÔ∏è –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–ª–æ–∂–µ–Ω–∏–µ")
            positive_flags += 1
        if features.get('has_percentages', 0) > 0:
            positive_points.append("üìä –ü—Ä–∏–≤–µ–¥–µ–Ω—ã —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ")
            positive_flags += 1
        if features.get('news_source_score', 0) > 0.2:
            positive_points.append("üèõÔ∏è –£–ø–æ–º–∏–Ω–∞—é—Ç—Å—è –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏")
            positive_flags += 1
        if features.get('credibility_indicator_score', 0) > 0.2:
            positive_points.append("üî¨ –ù–∞—É—á–Ω—ã–π/–∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥")
            positive_flags += 1
        if features.get('has_dates', 0) > 0:
            positive_points.append("üìÖ –£–∫–∞–∑–∞–Ω—ã –¥–∞—Ç—ã/—Å—Ä–æ–∫–∏")
            positive_flags += 1
        if features.get('has_currency', 0) > 0:
            positive_points.append("üí∞ –£–∫–∞–∑–∞–Ω—ã —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ")
            positive_flags += 1
        
        # –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        warning_points = []
        if features.get('clickbait_score', 0) > 0.3:
            clickbait_words = features.get('clickbait_words', [])[:3]
            clickbait_str = ", ".join(clickbait_words) if clickbait_words else "—ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞"
            warning_points.append(f"üö® –ö–ª–∏–∫–±–µ–π—Ç-—Å–ª–æ–≤–∞: {clickbait_str}")
            warning_flags += 1
            total_flags += 1
        if features.get('emotional_score', 0) > 0.5:
            warning_points.append("üò† –í—ã—Å–æ–∫–∞—è —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞")
            warning_flags += 1
            total_flags += 1
        if features.get('certainty_score', 0) > 0.4:
            certainty_words = features.get('certainty_words', [])[:2]
            certainty_str = ", ".join(certainty_words) if certainty_words else "–∫–∞—Ç–µ–≥–æ—Ä–∏—á–Ω—ã–µ —Å–ª–æ–≤–∞"
            warning_points.append(f"‚ö†Ô∏è –ò–∑–±—ã—Ç–æ—á–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—á–Ω–æ—Å—Ç—å: {certainty_str}")
            warning_flags += 1
            total_flags += 1
        if features.get('exclamation_density', 0) > 0.3:
            warning_points.append(f"‚ùó –í—ã—Å–æ–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏–π ({features['exclamation_count']} –∑–Ω–∞–∫–æ–≤)")
            warning_flags += 1
            total_flags += 1
        if features.get('caps_ratio', 0) > 0.2:
            warning_points.append("üî† –ú–Ω–æ–≥–æ –∑–∞–≥–ª–∞–≤–Ω—ã—Ö –±—É–∫–≤")
            warning_flags += 1
            total_flags += 1
        if features.get('intensifier_score', 0) > 0.4:
            warning_points.append("üí• –ú–Ω–æ–≥–æ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —É—Å–∏–ª–∏—Ç–µ–ª–µ–π")
            warning_flags += 1
            total_flags += 1
        if features.get('caps_lock_words', 0) > 1:
            warning_points.append(f"üÜò –°–ª–æ–≤–∞ –∑–∞–≥–ª–∞–≤–Ω—ã–º–∏: {features['caps_lock_words']} —à—Ç.")
            warning_flags += 1
            total_flags += 1
        if features.get('word_repetition_score', 0) > 0.4:
            common_words = features.get('most_common_words', [])[:3]
            if common_words:
                warning_points.append(f"üîÑ –ü–æ–≤—Ç–æ—Ä—ã —Å–ª–æ–≤: {', '.join(common_words)}")
                warning_flags += 1
                total_flags += 1
        
        # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞
        if warning_flags == 0 and positive_flags >= 3:
            explanations.append("‚úÖ –í—ã—Å–æ–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏")
            explanations.append("–¢–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –∂—É—Ä–Ω–∞–ª–∏—Å—Ç–∏–∫–∏")
        elif warning_flags <= 2 and positive_flags >= 2:
            explanations.append("‚ö†Ô∏è –°—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏")
            explanations.append("–¢–µ–∫—Å—Ç –∏–º–µ–µ—Ç –∫–∞–∫ —Å–∏–ª—å–Ω—ã–µ, —Ç–∞–∫ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã")
        elif warning_flags >= 3:
            explanations.append("üö® –ù–∏–∑–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏")
            explanations.append("–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –º–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Ñ–µ–π–∫–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        else:
            explanations.append("üìä –ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞")
            explanations.append("–¢–µ–∫—Å—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —è–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –∏–ª–∏ —Ñ–µ–π–∫–æ–≤–æ—Å—Ç–∏")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ç–∞–ª–∏
        if warning_points:
            explanations.append("\nüö© **–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Ç—Ä–µ–≤–æ–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:**")
            explanations.extend([f"‚Ä¢ {point}" for point in warning_points])
        
        if positive_points:
            explanations.append("\n‚úÖ **–ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏:**")
            explanations.extend([f"‚Ä¢ {point}" for point in positive_points])
        
        # –û—Å–æ–±—ã–µ —Å–ª—É—á–∞–∏
        if features.get('clickbait_score', 0) > 0.3 and features.get('has_percentages', 0) > 0:
            explanations.append("\nüí° **–í–∞–∂–Ω–æ:** –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫, —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö.")
        
        if len(text) < 150:
            explanations.append("\nüìù **–í–Ω–∏–º–∞–Ω–∏–µ:** –¢–µ–∫—Å—Ç –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–π. –ö–æ—Ä–æ—Ç–∫–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è —á–∞—Å—Ç–æ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.")
        
        if features.get('complexity_score', 0) < 0.2:
            explanations.append("\nüìö **–ó–∞–º–µ—á–∞–Ω–∏–µ:** –¢–µ–∫—Å—Ç –æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç–æ–π. –°–ª–æ–∂–Ω—ã–µ —Ç–µ–º—ã –æ–±—ã—á–Ω–æ —Ç—Ä–µ–±—É—é—Ç –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∏–∑–ª–æ–∂–µ–Ω–∏—è.")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = []
        if warning_flags > positive_flags:
            recommendations.append("üîç –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö")
            recommendations.append("üì∞ –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —Å—Ç–∏–ª—å –∏–∑–ª–æ–∂–µ–Ω–∏—è - –æ–Ω –º–æ–∂–µ—Ç –±—ã—Ç—å –º–∞–Ω–∏–ø—É–ª—è—Ç–∏–≤–Ω—ã–º")
        elif positive_flags > 0:
            recommendations.append("‚úÖ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤—ã–≥–ª—è–¥–∏—Ç –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω–æ, –Ω–æ –≤—Å–µ–≥–¥–∞ –ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ —Ñ–∞–∫—Ç—ã")
        
        if recommendations:
            explanations.append("\nüéØ **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**")
            explanations.extend([f"‚Ä¢ {rec}" for rec in recommendations])
        
        return explanations
    
    def assess_credibility(self, features, text):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏"""
        assessment = {
            'style_analysis': {},
            'content_analysis': {},
            'risk_factors': [],
            'confidence_level': 'medium'
        }
        
        # –ê–Ω–∞–ª–∏–∑ —Å—Ç–∏–ª—è
        style_score = 0
        max_style_score = 6
        
        if features.get('formality_score', 0) > 0.3:
            style_score += 1
            assessment['style_analysis']['formality'] = '–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç'
        
        if features.get('balance_score', 0) > 0.2:
            style_score += 1
            assessment['style_analysis']['balance'] = '–°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π'
        
        if features.get('exclamation_density', 0) < 0.3:
            style_score += 1
            assessment['style_analysis']['punctuality'] = '–£–º–µ—Ä–µ–Ω–Ω–∞—è'
        
        if features.get('caps_ratio', 0) < 0.2:
            style_score += 1
            assessment['style_analysis']['case_usage'] = '–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ'
        
        if features.get('lexical_diversity', 0) > 0.6:
            style_score += 1
            assessment['style_analysis']['vocabulary'] = '–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–π'
        
        if features.get('complexity_score', 0) > 0.3:
            style_score += 1
            assessment['style_analysis']['complexity'] = '–î–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è'
        
        assessment['style_analysis']['score'] = f"{style_score}/{max_style_score}"
        assessment['style_analysis']['percentage'] = round((style_score / max_style_score) * 100)
        
        # –ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
        content_score = 0
        max_content_score = 6
        
        if features.get('source_indicator_score', 0) > 0.2:
            content_score += 1
            assessment['content_analysis']['sources'] = '–ï—Å—Ç—å —Å—Å—ã–ª–∫–∏'
        
        if features.get('has_percentages', 0) > 0:
            content_score += 1
            assessment['content_analysis']['data'] = '–ï—Å—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞'
        
        if features.get('has_dates', 0) > 0:
            content_score += 1
            assessment['content_analysis']['dates'] = '–ï—Å—Ç—å –¥–∞—Ç–∏—Ä–æ–≤–∫–∞'
        
        if features.get('news_source_score', 0) > 0.2:
            content_score += 1
            assessment['content_analysis']['official_sources'] = '–£–ø–æ–º—è–Ω—É—Ç—ã'
        
        if features.get('credibility_indicator_score', 0) > 0.2:
            content_score += 1
            assessment['content_analysis']['methodology'] = '–ù–∞—É—á–Ω—ã–π –ø–æ–¥—Ö–æ–¥'
        
        if features.get('has_currency', 0) > 0:
            content_score += 1
            assessment['content_analysis']['financial_data'] = '–ï—Å—Ç—å'
        
        assessment['content_analysis']['score'] = f"{content_score}/{max_content_score}"
        assessment['content_analysis']['percentage'] = round((content_score / max_content_score) * 100)
        
        # –§–∞–∫—Ç–æ—Ä—ã —Ä–∏—Å–∫–∞
        if features.get('clickbait_score', 0) > 0.4:
            assessment['risk_factors'].append({
                'factor': '–ö–ª–∏–∫–±–µ–π—Ç',
                'severity': 'high' if features['clickbait_score'] > 0.6 else 'medium',
                'description': '–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è'
            })
        
        if features.get('certainty_score', 0) > 0.5:
            assessment['risk_factors'].append({
                'factor': '–ö–∞—Ç–µ–≥–æ—Ä–∏—á–Ω–æ—Å—Ç—å',
                'severity': 'high',
                'description': '–ò–∑–±—ã—Ç–æ—á–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è—Ö'
            })
        
        if features.get('exclamation_density', 0) > 0.5:
            assessment['risk_factors'].append({
                'factor': '–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å',
                'severity': 'high',
                'description': '–í—ã—Å–æ–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏–π'
            })
        
        if features.get('caps_ratio', 0) > 0.3:
            assessment['risk_factors'].append({
                'factor': '–ö—Ä–∏—á–∞—â–∏–π —Å—Ç–∏–ª—å',
                'severity': 'medium',
                'description': '–ú–Ω–æ–≥–æ –∑–∞–≥–ª–∞–≤–Ω—ã—Ö –±—É–∫–≤'
            })
        
        # –£—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        total_score = style_score + content_score
        max_total_score = max_style_score + max_content_score
        confidence_percentage = (total_score / max_total_score) * 100
        
        if confidence_percentage >= 70:
            assessment['confidence_level'] = 'high'
        elif confidence_percentage >= 40:
            assessment['confidence_level'] = 'medium'
        else:
            assessment['confidence_level'] = 'low'
        
        assessment['overall_score'] = round(confidence_percentage)
        
        return assessment
